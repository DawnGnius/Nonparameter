\begin{problem}
    Let $X_1,\ldots,X_n~iid\sim F$, $F_n$ be the empirical distribution function and $a<b$ be fixed real numbers, Let $\theta=T(F)=F(b)-F(a)$.  

    (1) Find $\theta$'s plug-in estimator $\hat\theta$;

    (2) Find the influence function and empirical influence function of $\theta$;

    (3) Estimate the standard error for $\hat\theta$;

    (4) Find an expression for an approximate $1-\alpha$ confidence interval for $\theta$. 
\end{problem}

\begin{solution}
    (1) $\theta = T(F) = F(b) - F(a) = \int I(a < x \le b) dF(x)$.

    The plug-in estimator is $\hat{\theta} =  \int I(a < x \le b) d F_n(x) = \frac{1}{n}\sum_{i=1}^n I(a < X_i \le b)$.

    (2) $\theta$ is a linear functional. Thus the influence function is $\text{IF}(x) = I(a < x \le b) - T(F)$ and the empirical influence function is  $\widehat{\text{IF}}(x) = I(a < x \le b) - T(F_n)$.

    (3) Denote the estimated standard error of $\hat{\theta}$ by  $\widehat{\textbf{se}}$. 

    \small{\emph{Note that, it cannot be obtained by doing some direct calculations. Because $\mathbb{V} \left( \sum_{i=1}^n I(a < X_i \le b) \right) = \sum_{i=1}^n \mathbb{V} (I(a < X_i \le b)) = n \times (F(b)-F(a)) \times (1-F(b)+F(a))$ is still undetermined. So we shell use influence function to get estimation of variance.}}
    
    \begin{equation*}
        \hat{\tau}^2 = \frac{1}{n} \sum_{i=1}^n \widehat{\text{IF}}^2 (X_i) =  \frac{1}{n} \sum_{i=1}^n \left(I(a < X_i \le b) -  \frac{1}{n}\sum_{j=1}^n I(a < X_j \le b) \right)^2 .
    \end{equation*}
    Then $\widehat{\textbf{se}} = \hat{\tau}/\sqrt{n}$, according to Theorem 2.22 in \citet{Wasserman2006All}.

    (4) Using Theorem 2.22 in \citet{Wasserman2006All}, we have 
    \begin{equation*}
        \frac{\sqrt{n} ( T(F) - F(F_n))}{\hat{\tau}} \leadsto N(0, 1).
    \end{equation*}
    
    Thus, a $1-\alpha$, pointwise asymptotic confidence interval for $\theta = T(F)$ is
    \begin{equation*}
        T(F_n) \pm z_{\alpha/2} \widehat{\textbf{se}} ,
    \end{equation*}
    where $\widehat{\textbf{se}}$ is calculated in (3).
\end{solution}




\begin{problem}
    Let $b(\epsilon)=\sup_x|T(F)-T(F_\epsilon)|$, $F_\epsilon=(1-\epsilon)F+\epsilon\delta_x$. A breakdown point of estimator $\epsilon^*$ is definded as $\epsilon^*=\inf\{\epsilon> 0: b(\epsilon)=\infty\}$. Find

    (1) Breakdown point of mean;

    (2) Breakdown point of median. 
\end{problem}

\begin{solution}
    (1) 
    Firstly, we analyze the problem from a mathematical point of view. 
    Let $T = T(F) = \int x dF$. 
    Then $T(F) - T(F_\epsilon) = \int x d (F - F_\epsilon) = \int x d (\epsilon F - \epsilon\delta_x) =  \epsilon T(F) - \epsilon x $.
    So $\sup_x|T(F)-T(F_\epsilon)| = \sup_x \epsilon  | T(F) - x | = \epsilon  \sup_x | T(F) - x | $ . 
    
    We have that, $\forall \epsilon >0 $, $\sup_x|T(F)-T(F_\epsilon)| = \infty$. 
    Thus $\epsilon^* = 0$.

    Then, intuitively\citep{Charles2006}, it is obvious from the formula from the mean 
    \begin{equation*}
        \frac{x_1 + \dots + x_n}{n}
    \end{equation*}
    that if we hold $x_1,\dots, x_{n-1}$ fixed and let $x_n$ go to infinity, the sample mean also goes to infinity. 
    In short  even one gross outlier ruins the sample mean. The finite sample breakdown point is $1/n$. The asymptotic breakdown point is zero.

    (2) Let $T = T(F) = F^{-1}(1/2) = \inf\{\mu | F(\mu) \ge 1/2 \}$. 
    We have 
    \begin{equation*}
        \begin{split}
            T(F_\epsilon) =  & F^{-1}_\epsilon (1/2) = \inf \left\{ \mu | (1-\epsilon)F(\mu) + \epsilon \delta_x(\mu) \ge 1/2 \right\} \\
            & = \left\{
            \begin{split}
                F^{-1} (1/2(1-\epsilon)) , \quad & x > F^{-1} (1/2(1-\epsilon))\\
                x, \quad &  F^{-1} ((1/2 - \epsilon)/(1-\epsilon)) < x \le F^{-1} (1/2(1-\epsilon))\\
                F^{-1} ((1/2 - \epsilon)/(1-\epsilon)) , \quad & x \le F^{-1} ((1/2 - \epsilon)/(1-\epsilon))\\
            \end{split}    
            \right.
        \end{split}
    \end{equation*}
    So $\sup_x|T(F)-T(F_\epsilon)| = \infty$ when $\epsilon=1/2$. Thus $\epsilon^* = 1/2$.

Intuitively\citep{Charles2006}, if we have n data points and we let a minority of them $\text{floor}((n - 1)/2)$ go to infinity leaving the rest fixed, the ``floor'' operation means largest integer less than or equal to, then the median stays with the majority. The median changes, but does not become arbitrarily bad. The finite sample breakdown point is $\text{floor}((n - 1)/2n)$. The asymptotic breakdown point is one-half.
\end{solution}





\begin{problem}
    Let $X$ be positive random variable with distribution function $F$ and let $\theta = \int log(x) d F(x)$, $\lambda = log (\mu), \mu = EX$. 

    (1) Find the influence function and empirical influence function of $\theta,\lambda$;

    (2) Do $\hat\theta,\hat\lambda$ have the same limiting distribution?

    (3) Who is more robust to outliers in $\hat\theta$ and $\hat\lambda$?
\end{problem}

\begin{solution}
    (1) $\theta = T_\theta(F) = \int log(x) d F(x)$ is a linear functional, so the influence function is $\text{IF}_\theta = log(x) - T_\theta(F)$ and the empirical influence function is $\widehat{\text{IF}}_\theta = log(x) - T_\theta(F_n)$ .

    $\lambda = T_\lambda(F) = log(\int x dF)$ is not a linear functional. So 
    \begin{equation*}
        \begin{split}
            \text{IF}_\lambda & = \lim_{\epsilon\to0} \frac{T_\lambda((1-\epsilon)F + \epsilon \delta_x) - T_\lambda(F)}{\epsilon} \\
            & = \lim_{\epsilon\to0}  \frac{log(\int x d ((1-\epsilon)F + \epsilon \delta_x) - log(\int x dF)}{\epsilon} \\
            & = \lim_{\epsilon\to0}  \frac{log \left( \epsilon  x /\int x dF + 1-\epsilon \right)}{\epsilon} \\
            & = \frac{x}{\int x dF} - 1 .
        \end{split}
    \end{equation*}
    And the empirical influence function is $\widehat{\text{IF}}_\lambda = \frac{x}{\int x d F_n} - 1$.

    (2) $\hat{\theta} = T_\theta(\hat{F}_n(x))$ and $\hat{\lambda} = T_\lambda(\hat{F}_n(x))$ are plug-in estimators of $\theta$ and $\lambda$ respectively. 
    First, we consider $\hat{\theta}$. 
    We have
    \begin{equation*}
        % \frac{\sqrt{n} (T_\theta - T(\hat{F_n}))}{\hat{\tau}} \leadsto N(0, 1)
        \sqrt{n} (T_\theta - T_\theta(\hat{F_n})) \leadsto N(0, \tau_\theta^2)
    \end{equation*}
    where $\tau_\theta^2 = \int \text{IF}_\theta(x) d F(x)$. 
    % where
    % \begin{equation*}
    %     \hat{\tau}_\theta = \frac{1}{n} \sum_{i=1}^n \hat{L}^2(X_i)
    %     = \frac{1}{n} \sum_{i=1}^n (log(X_i) - T_\theta(\hat{F_n}))^2
    % \end{equation*}
    The limiting distribution of $\hat{\theta}$ is $N(T_\theta, \tau_\theta^2/n)$

    Then, for $\hat{\lambda}$,
    \begin{equation*}
        \sqrt{n} (T_\lambda - T_\lambda(\hat{F_n})) \leadsto N(0, \tau_\lambda^2)
    \end{equation*}
    where $\tau_\lambda^2 = \int \text{IF}_\lambda(x) d F(x)$. 
    The limiting distribution of $\hat{\lambda}$ is $N(T_\lambda, \tau_\lambda^2/n)$
    
    They are different.

    (3) Technically, we use breakdown points to measure the robustness of a statistic to outliers. Thus we consider the breakdown points of those two estimators.
    
    For $\hat{\theta}$, 
    \begin{equation*}
        T_\theta(F) - T_\theta(F_\epsilon) = \int log(x) d \epsilon (F -\delta_x)
        = \epsilon (T_\theta - log(x)).
    \end{equation*}
    Then
    \begin{equation*}
        \sup_x | T_\theta(F) - T_\theta(F_\epsilon) | = \epsilon |T_\theta - log(x)|
    \end{equation*}

    Thus, breakdown point $\epsilon^* = 0$.

    For $\hat{\lambda}$, 
    \begin{equation*}
        T_\lambda(F) - T_\lambda(F_\epsilon) = log \left( \int x d F \right) - log \left( \int x d( (1-\epsilon)F + \epsilon \delta_x)\right)
        % = log( \frac{ \int x d F}{\int x d( (1-\epsilon)F + \epsilon \delta_x)})
        = log\left(\frac{\mu}{(1-\epsilon)\mu + \epsilon x}\right)
    \end{equation*}
    Then
    \begin{equation*}
        \sup_x | T_\lambda(F) - T_\lambda(F_\epsilon) | 
        =  \infty,
        \quad \text{when } x = \frac{(\epsilon - 1)\mu}{\epsilon} \text{ or } \infty.
    \end{equation*}

    Thus, breakdown point $\epsilon^* = 0$.

    So, from the breakdown point characterization of robustness, they are both sensitive to outliers. 
    But, intuitively, log transformation can reduce the influence of outliers before taking a average of samples. 
    
    Thus I suppose $\theta= \int log(x) d F(x)$ is more robust to outliers than $\lambda = log \left( \int  x d F(x) \right)$.

\end{solution}
