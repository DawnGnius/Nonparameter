\begin{problem}
Let $X_1,\ldots,X_m i.i.d\sim F$, $Y_1,\ldots,Y_n i.i.d \sim G$ and i.i.d within each sample, then

(1) get the U-statistic $U_n$ with kernel $h(x_1,x_2,y_1,y_2)=I(x_1<y_1,x_2<y_2)$,

(2) get the limit distribution of U-statistic $U_n$ with $m+n\rightarrow\infty, \frac{m}{n+m}\rightarrow p\in (0,1)$,

(3) get the limit distribution of U-statistic $U_n$ under null hypothesis $H_0:F=G$.

\end{problem}


\begin{solution}
(1) The kerne is $h(x_1,x_2,y_1,y_2)=I(x_1<y_1,x_2<y_2)$, which is of order 2 in both $x$ and $y$. The corresponding U-statistic is
\begin{equation*}
    U_n=\frac{1}{\left( \begin{matrix} m \\ 2 \end{matrix} \right) \left( \begin{matrix} n \\ 2 \end{matrix} \right)} \sum_{i<k}^m \sum_{j<l}^n  I(X_i<Y_j,X_k<Y_l)
\end{equation*}

(2) The projection of $U-\theta$ onto the set of all functions of the form $\sum_{i=1}^m k_i (X_i) + \sum_{j=1}^n l_j(Y_j)$ is given by 
\begin{equation*}
    \hat{U} = \frac{2}{m} \sum_{i=1}^m h_{1,0} (X_i) + \frac{2}{n} \sum_{j=1}^n h_{0,1} (Y_j),
\end{equation*}
where the functions $h_{1,0}$ and $h_{0,1}$ are defined by
\begin{equation*}
    \begin{split}
        h_{1,0}(x) & = Eh(x,X_2,Y_1,Y_2) - \theta, \\
        h_{0,1}(y) & = Eh(X_1,X_2,y,Y_2) - \theta. \\
    \end{split}
\end{equation*}

The sequence $\hat{U}$ is asymtotically normal by the central limit theorem. Then difference between $\hat{U}$ and $U_n-\theta$ is asymtotically negligible. 

% If the $X_i$ and $Y_j$ have cumulative distribution functions $F$ and $G$,
\begin{equation*}
    \begin{split}
        \theta & = Eh(X_1, X_2, Y_1, Y_2)  \\
        & =  P(X_1 <Y_1)P(X_2<Y_2) \\
        & = (E [P(X_1 < y | Y_1 = y)] )^2 \\
        & =  (E [F(Y_1)])^2 \\
        & = \left(\int F(y) d G(y) \right)^2
    \end{split}
\end{equation*}

Then calculate $\zeta_{1,0}$ and $\zeta_{0,1}$.
% \begin{equation*}
%     \begin{split}
%         h_{1,0}(x) &  = Eh(x,X_2,Y_1,Y_2) - \theta, \\
%         & = P(Y_1 > x) P(X_2 < Y_2) \\
%         & = [1-G(x)] \int F(y) d G(y)
%     \end{split}
% \end{equation*}
% and
% \begin{equation*}
%     \begin{split}
%         h_{0,1}(y) & = Eh(X_1,X_2,y,Y_2) - \theta. \\
%         & = P(X1 < y) P(X_2 < Y_2) \\
%         & = F(y) \int F(y) d G(y)
%     \end{split}
% \end{equation*}

% Thus
% \begin{equation*}
%     \begin{split}
%         \hat{U} & = \frac{2}{m} \sum_{i=1}^m h_{1,0} (X_i) + \frac{2}{n} \sum_{j=1}^n h_{0,1} (Y_j) \\
%         & =  \frac{2}{m} \sum_{i=1}^m h_{1,0} (X_i) + \frac{2}{n} \sum_{j=1}^n h_{0,1} (Y_j) \\
%     \end{split}
% \end{equation*}

\begin{equation*}
    \begin{split}
        \zeta_{1,0} & = cov(h(X_1, X_2, Y_1, Y_2), h(X_1. X_3, Y_3, Y_4)) \\
        & = cov(I(X_1<Y_1, X_2<Y_2), I(X_1<Y_3, X_3<Y_4)) \\
        & = E[I(X_1<Y_1, X_2<Y_2)I(X_1<Y_3, X_3<Y_4)] \\ 
        & \quad \quad \quad - E[I(X_1<Y_1, X_2<Y_2)] E[I(X_1<Y_3, X_3<Y_4)] \\
        & = E[I(X_1< \min(Y_1, Y_3)) I(X_2<Y_2) I(X_3 < Y_4) ] \\
        & \quad \quad \quad - E[I(X_1<Y_1) I(X_2<Y_2)] E[I(X_1<Y_3) I(X_3<Y_4)] \\
        & = P(X_1< \min(Y_1, Y_3))(P(X_1<Y_1))^2 - (P(X_1<Y_1))^4  \\
        & = \left(\int F(y) d G(y) \right)^2 \int F(z) d(-G^2(z)+2G(z)) - \left(\int F(y) d G(y) \right)^4
    \end{split}
\end{equation*}
and 
\begin{equation*}
    \begin{split}
        \zeta_{0,1} & = cov(h(X_1, X_2, Y_1, Y_2), h(X_3. X_4, Y_1, Y_3)) \\
        & = cov(I(X_1<Y_1, X_2<Y_2), I(X_3<Y_1, X_4<Y_3)) \\
        & = E[I(X_1<Y_1, X_2<Y_2)I(X_3<Y_1, X_4<Y_3)] \\ 
        & \quad \quad \quad - E[I(X_1<Y_1, X_2<Y_2)] E[I(X_3<Y1, X_4<Y_3)] \\
        & = E[I(Y_1 > \max(X_1, X_3)) I(Y_2 > X_2) I(Y_3 > X_4)] \\
        & \quad \quad \quad - E[I(X_1<Y_1) I(X_2<Y_2)] E[I(X_3<Y_1) I(X_4<Y_3)] \\
        & = P(Y_1 > \max(X_1, X_3))(P(X_1<Y_1))^2 - (P(X_1<Y_1))^4  \\
        & = \left(\int F(y) d G(y) \right)^2 \int F^2(z) dG(z) - \left(\int F(y) d G(y) \right)^4
    \end{split}
\end{equation*}

Thus,
\begin{equation*}
    \sqrt{m+n}(U_n-\theta) \leadsto N \left(0, \frac{4\zeta_{1,0}}{p}+\frac{4\zeta_{0,1}}{1-p} \right)
\end{equation*}
where $\theta=\left(\int F(y) d G(y) \right)^2$, $\zeta_{1,0}=\left(\int F(y) d G(y) \right)^2 \int F(z) d(-G^2(z)+2G(z)) - \left(\int F(y) d G(y) \right)^4$ and $\zeta_{0,1}=\left(\int F(y) d G(y) \right)^2 \int F^2(z) dG(z) - \left(\int F(y) d G(y) \right)^4$.


(3) Under $H_0: F=G$, $\theta=1/4$, $\zeta_{1,0}=\frac{1}{4*12}$ and $\zeta_{0,1}=\frac{1}{4*12}$.
\begin{equation*}
    \sqrt{m+n} \left(U_n-\frac{1}{2} \right) \leadsto N \left(0, \frac{1}{12p}+\frac{1}{12(1-p)} \right)
\end{equation*}
\end{solution}




\begin{problem}
    Suppose the distribution of $X$ is symmetric about zero with variance $\sigma^2>0$ and $EX^4 < \infty$, consider kernel $h(x,y)=xy+(x^2-\sigma^2)(y^2-\sigma^2)$, then

(1) prove that the U-statistic $U_n$ with kernel $h(x,y)$ has a degeneracy of order $1$,

(2) get $\lambda_1, \lambda_2$ and orthogonal functions $\Phi_1(x), \Phi_2(x)$, such that $h(x,y)=\lambda_1\varphi_1(x)\varphi_1(y)+\lambda_2\varphi_2(x)\varphi_2(y)$,

(3) get the limit distribution of $n U_n$.
\end{problem}


\begin{solution}
(1)
Firstly, we obtain U-statistic $U_n$,
\begin{equation*}
    U_n = \frac{1}{\left(\begin{matrix} n \\ 2  \end{matrix}\right)} \sum_{i<j}^n X_iX_j + (X_i^2-\sigma^2)(X_j^2-\sigma^2)
\end{equation*}
with $X_1, \dots,X_n\, i.i.d$  and $E[X_i]=\mu=0, E[X_i^3]=0$.

Then, we obtain $\theta$,
\begin{equation*}
    \begin{split}
        \theta & = E[h(X_1, X_2)]  \\
        & = E [ X_1X_2 + (X_1^2-\sigma^2)(X_2^2-\sigma^2) ] \\
        & = \mu^2 + \mu^4.
    \end{split}
\end{equation*}
Therefore, $U_n$ is an unbiased estimator of $\mu^2 + \mu^4$.

To prove the U-statistic $U_n$ with kernel $h(x,y)$ has a degeneracy of order $1$, it's sufficient to show that $\zeta_1=0$ and $\zeta_2> 0$.

Since $h_1(x) = E[xX_2 + (x^2-\sigma^2)(X_2^2-\sigma^2)] = x\mu + (x^2-\sigma^2)\mu^2$ and
\begin{equation*}
    \begin{split}
        \zeta_1 & = \var[h_1(X_1)] = \mu^2 (\var(X_1 + \mu(X_1^2-\sigma^2))) = 0 \\
        \zeta_2 & = \var[h(X_1, X_2)] = \var[X_1X_2 + (X_1^2-\sigma^2)(X_2^2-\sigma^2)] \\
        & = \sigma^4 + \left( E[X_1^4] - \sigma^4 \right)^2 > 0 
    \end{split}
\end{equation*}
so that the degeneracy is of order 1.

(2) We  take $A(x_1, x_2) = h(x_1, x_2) - \theta$, where $\theta = E h(X_1, X_2) = 0$.
For $k=1,2$, 
\begin{equation}\label{eq:A}
    E[A(x,X_2)\phi_k(X_2)] = \lambda_k \phi_k(x).
\end{equation}

We choose $\phi_1(x) = x/\sigma, \phi_2(x) = (x^2-\sigma^2)/\sqrt{EX^4-\sigma^4}$, which satisfy $E\phi_j(X_1)=0$ and 
\begin{equation*}
    E\phi_j(X_1)\phi_k(X_1) = \left\{
    \begin{aligned}
        1, & \quad\quad j=k \\
        0, & \quad\quad j\neq k
    \end{aligned}
    \right.
\end{equation*}
So $\phi_1$ is orthogonal with $\phi_2$. Thus $\lambda_1 = \sigma^2, \lambda_2=EX^4-\sigma^4$ by \eqref{eq:A}.

(3) $U_n$ is the U-statistic associated with a symmetric kernel of degree 2, degeneracy of order $1$ and expectation $0$. Then
\begin{equation*}
        nU_n \leadsto \sigma^2(Z_1^2-1) + (EX^4-\sigma^4)(Z_2^2-1),
\end{equation*}
where $Z_1, Z_2$ are independent $N(0,1)$. 

\end{solution}




\begin{problem}
    Prove the Hoeffding decomposition in page 13.
\end{problem}


\begin{solution}
    
\end{solution}




\begin{problem}
    Prove the T decomposition in page 12.
\end{problem}


\begin{solution}
    
\end{solution}




