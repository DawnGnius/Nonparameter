Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{sheather1991reliable,
author = {Sheather, Simon J and Jones, Michael C},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
number = {3},
pages = {683--690},
publisher = {Wiley Online Library},
title = {{A reliable data-based bandwidth selection method for kernel density estimation}},
volume = {53},
year = {1991}
}
@article{Yu2015,
abstract = {This paper studies semiparametric efficient estimation of the threshold point in threshold regression. The classical literature of semiparametric efficient estimation rests on the fact that the maximum likelihood estimator is efficient in any parametric submodel for a large class of loss functions. However, in threshold regression, the maximum likelihood estimator is not efficient, while the Bayes estimators are efficient and different loss functions induce different efficient estimators. For an additively separable loss function that separates the efficiency problem of the threshold point from that of other parameters, we show that the semiparametric and parametric efficiency risk bounds coincide. Then we design a semiparametric empirical Bayes estimator to achieve this bound. In consequence, the threshold point can be adaptively estimated even under conditional moment restrictions. We also provide a valid confidence interval called the nonparametric posterior interval for the threshold point. Simulation studies show that the semiparametric empirical Bayes approach is substantially better than existing methods. To illustrate our procedure in practice, we apply it to an economic growth model for detecting different growth patterns.},
author = {Yu, Ping},
doi = {10.1016/j.jeconom.2013.09.002},
file = {::},
issn = {18726895},
journal = {Journal of Econometrics},
keywords = {Adaptive estimation,Additively separable loss function,Compound Poisson process,Curse of dimensionality,Middle-point LSE,Nonparametric posterior interval,Nonregular model,Semiparametric efficiency,Semiparametric empirical Bayes,Threshold regression},
number = {1},
pages = {83--100},
publisher = {Elsevier B.V.},
title = {{Adaptive estimation of the threshold point in threshold regression}},
url = {http://dx.doi.org/10.1016/j.jeconom.2013.09.002},
volume = {189},
year = {2015}
}
@article{Julious2001,
abstract = {Thw two-line model when the location of the changepoint is known is introduced, with an F-test to detect a change in the regression coefficient. The situation when the changepoint is unknown is then introduced and an algorithm proposed for parameter estimation. It is demonstrated that when the location of the changepoint is not known the F-test does not conform to its expected parametric distribution. Nonparametric bootstrap methods are proposed as a way of overcoming the problems encountered. Finally, a physiology example is introduced where the regression change represents the change from aerobic to anaerobic energy production.},
author = {Julious, Steven A.},
doi = {10.1111/1467-9884.00260},
file = {::},
issn = {0039-0526},
journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
keywords = {bootstrapping,changepoint regression,least squares estimates,piecewise regression,split lines,two-phase regression},
number = {1},
pages = {51--61},
title = {{Inference and Estimation in a Changepoint Regression Problem}},
url = {http://doi.wiley.com/10.1111/1467-9884.00260},
volume = {50},
year = {2001}
}
@article{friedman1984projection,
author = {Friedman, Jerome H and Stuetzle, Werner and Schroeder, Anne},
journal = {Journal of the American Statistical Association},
number = {387},
pages = {599--608},
publisher = {Taylor {\&} Francis Group},
title = {{Projection pursuit density estimation}},
volume = {79},
year = {1984}
}
@misc{Lafferty2008,
abstract = {We present a greedy method for simultaneously performing local bandwidth selection and variable selection in nonparametric regression. The method starts with a local linear estimator with large bandwidths, and incrementally decreases the bandwidth of variables for which the gradient of the estimator with respect to bandwidth is large. The method - called rodeo (regularization of derivative expectation operator) - conducts a sequence of hypothesis tests to threshold derivatives. and is easy to implement. Under certain assumptions on the regression function and sampling density, it is shown that the rodeo applied to local linear smoothing avoids the curse of dimensionality. achieving near optimal minimax rates of convergence in the number of relevant variables, as if these variables were isolated in advance. {\textcopyright} Institute of Mathematical Statistics, 2008.},
author = {Lafferty, John and Wasserman, Larry},
booktitle = {Annals of Statistics},
doi = {10.1214/009053607000000811},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Rodeo{\_}Sparse, greedy nonparametric regression.pdf:pdf},
issn = {00905364},
keywords = {Bandwidth estimation,Local linear smoothing,Minimax rates of convergence,Nonparametric regression,Sparsity,Variable selection},
number = {1},
pages = {28--63},
title = {{Rodeo: Sparse, greedy nonparametric regression}},
volume = {36},
year = {2008}
}
@article{Tecuapetla-Gomez2017,
abstract = {We discuss a class of difference-based estimators for the autocovariance in nonparametric regression when the signal is discontinuous (change-point regression), possibly highly fluctuating, and the errors form a stationary {\$}m{\$}-dependent process. These estimators circumvent the explicit pre-estimation of the unknown regression function, a task which is particularly challenging for such signals. We provide explicit expressions for their mean squared errors when the signal function is piecewise constant (segment regression) and the errors are Gaussian. Based on this we derive biased-optimized estimates which do not depend on the particular (unknown) autocovariance structure. Notably, for positively correlated errors, that part of the variance of our estimators which depends on the signal is minimal as well. Further, we provide sufficient conditions for {\$}\backslashsqrt{\{}n{\}}{\$}-consistency; this result is extended to piecewise Holder regression with non-Gaussian errors. We combine our biased-optimized autocovariance estimates with a projection-based approach and derive covariance matrix estimates, a method which is of independent interest. Several simulation studies as well as an application to biophysical measurements complement this paper.},
archivePrefix = {arXiv},
arxivId = {1507.02485},
author = {Tecuapetla-G{\'{o}}mez, Inder and Munk, Axel},
doi = {10.1111/sjos.12256},
eprint = {1507.02485},
file = {::},
issn = {14679469},
journal = {Scandinavian Journal of Statistics},
keywords = {autocovariance estimation,change-point,convex projection,covariance matrix estimation,difference-based methods,discontinuous signal,m-dependent processes,mean squared error,non-parametric regression},
number = {2},
pages = {346--368},
title = {{Autocovariance Estimation in Regression with a Discontinuous Signal and m-Dependent Errors: A Difference-Based Approach}},
volume = {44},
year = {2017}
}
@incollection{Shi2007,
address = {Beijing, China},
booktitle = {Computational Science - ICCS 2007},
editor = {Shi, Yong and van Albada, Geert Dick and Dongarra, Jack},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/2007{\_}ICCS{\_}Computation Science.pdf:pdf},
isbn = {978-3-540-72587-9},
title = {{2007{\_}ICCS{\_}Computation Science.pdf}},
year = {2007}
}
@article{Osorio2006,
author = {Osorio, Felipe and Galea, Manuel},
doi = {10.1007/s00362-005-0271-x},
file = {::},
issn = {09325026},
journal = {Statistical Papers},
keywords = {Change-point,Regression model,Schwarz information criterion,Student-t model},
number = {1},
pages = {31--48},
title = {{Detection of a change-point in student-t linear regression models}},
volume = {47},
year = {2006}
}
@misc{Lukasik2007,
abstract = {Kernel density estimation is nowadays a very popular tool for nonparametric probabilistic density estimation. One of its most important disadvantages is computational complexity of calculations needed, especially for data-based bandwidth selection and adaptation of bandwidth coefficient. The article presents parallel methods which can significantly improve calculation time. Results of using reference implementation based on Message Passing Interface standard in multicomputer environment are included as well as a discussion on effectiveness of parallelization. {\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
author = {Lukasik, Szymon},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-72588-6_120},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Parallel Computing of Kernel Density Estimates with MPI.pdf:pdf},
isbn = {9783540725879},
issn = {03029743},
keywords = {Adaptive bandwidth,Kernel density estimation,Least squares cross-validation,MPI,Parallel algorithms,Plug-in method},
number = {PART 3},
pages = {726--733},
title = {{Parallel computing of kernel density estimates with MPI}},
volume = {4489 LNCS},
year = {2007}
}
@article{Peng2011,
abstract = {This note revisits Maity and Sherman's two-sample testing problem with one variance known but the other one unknown [A. Maity, M. Sherman, The two-sample t test with one variance unknown, The American Statistician 60 (2006) 163-166]. Inspired by the fact that the number of degrees of freedom used in their testing method is overestimated, we propose in this note a new testing method by introducing an unbiased estimator of the number of degrees of freedom. Simulation studies indicate that the proposed testing method provides a more accurate control than Maity and Sherman's method. {\textcopyright} 2011 Elsevier B.V.},
author = {Peng, Liqian and Tong, Tiejun},
doi = {10.1016/j.stamet.2011.07.001},
file = {::},
issn = {15723127},
journal = {Statistical Methodology},
keywords = {Behrens-Fisher,Bias correction,Student's t distribution,Type I error,Welch-Satterthwaite approximation},
number = {6},
pages = {528--534},
publisher = {Elsevier B.V.},
title = {{A note on a two-sample T test with one variance unknown}},
url = {http://dx.doi.org/10.1016/j.stamet.2011.07.001},
volume = {8},
year = {2011}
}
@article{Agrawal2005,
abstract = {Data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate clusters in large high dimensional datasets.},
author = {Agrawal, Rakesh and Gehrke, Johannes and Gunopulos, Dimitrios and Raghavan, Prabhakar},
doi = {10.1007/s10618-005-1396-1},
file = {::},
isbn = {0-89791-995-5},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {Clustering,Dimensionality reduction,Subspace clustering},
number = {1},
pages = {5--33},
title = {{Automatic subspace clustering of high dimensional data}},
volume = {11},
year = {2005}
}
@article{Sun2010comments,
author = {Sun, T and Zhang, C-H.},
doi = {10.1007/s11749-010-0201-7},
file = {::},
journal = {Test},
number = {2},
pages = {270--275},
title = {{Comments on: {\{}{\$}\backslashell{\_}1{\$}{\}}-penalization for mixture regression models}},
volume = {19},
year = {2010}
}
@article{Weihs2016,
author = {Weihs, Claus and Herbrandt, Swetlana and Bauer, Nadja and Friedrichs, Klaus and Horn, Daniel},
file = {::},
number = {64},
title = {{Tests for scale changes based on pairwise differences}},
year = {2016}
}
@article{Hui2009,
author = {Hui, L I and Chang, T A N and Bai, M I A O},
file = {::},
title = {{Strong convergence rate for slope change point estimator}},
year = {2009}
}
@book{David2012,
address = {Kingston},
author = {Skillicorn, David B.},
doi = {10.1007/978-3-642-33398-9},
file = {::},
isbn = {978-3-642-33397-2},
issn = {2191-5776},
pages = {96},
publisher = {Springer Berlin Heidelberg},
series = {SpringerBriefs in Computer Science},
title = {{Understanding High-Dimensional Spaces}},
url = {http://link.springer.com/10.1007/978-3-642-33398-9},
year = {2012}
}
@article{Dai2015,
abstract = {Over the past three decades, interest in cheap yet competitive variance estimators in nonparametric regression has grown tremendously. One family of estimators which has risen to meet the task is the difference-based estimators. Unlike their residual-based counterparts, difference-based estimators do not require estimating the mean function and are therefore popular in practice. This work further develops the difference-based estimators in the repeated measurement setting for nonparametric regression models. Three difference-based methods are proposed for the variance estimation under both balanced and unbalanced repeated measurement settings: the sample variance method, the partitioning method, and the sequencing method. Both their asymptotic properties and finite sample performance are explored. The sequencing method is shown to be the most adaptive while the sample variance method and the partitioning method are shown to outperform in certain cases.},
author = {Dai, Wenlin and Ma, Yanyuan and Tong, Tiejun and Zhu, Lixing},
doi = {10.1016/j.jspi.2015.02.010},
file = {::},
isbn = {03783758},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Asymptotic normality,Difference-based estimator,Least squares,Nonparametric regression,Repeated measurements,Residual variance},
pages = {1--20},
publisher = {Elsevier B.V.},
title = {{Difference-based variance estimation in nonparametric regression with repeated measurement data}},
url = {http://dx.doi.org/10.1016/j.jspi.2015.02.010},
volume = {163},
year = {2015}
}
@inproceedings{zheng2013quality,
  title={Quality and efficiency for kernel density estimates in large data},
  author={Zheng, Yan and Jestes, Jeffrey and Phillips, Jeff M and Li, Feifei},
  booktitle={Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
  pages={433--444},
  year={2013},
  organization={ACM}
}
@misc{Bahadori2016SEED,
abstract = {Sparse reduced-rank regression is an important tool to uncover meaningful dependence structure between large numbers of predictors and responses in many big data applications such as genome-wide association studies and social media analysis. Despite the recent theoretical and algorithmic advances, scalable estimation of sparse reduced-rank regression remains largely unexplored. In this paper, we suggest a scalable procedure called sequential estimation with eigen-decomposition (SEED) which needs only a single top-{\$}r{\$} singular value decomposition to find the optimal low-rank and sparse matrix by solving a sparse generalized eigenvalue problem. Our suggested method is not only scalable but also performs simultaneous dimensionality reduction and variable selection. Under some mild regularity conditions, we show that SEED enjoys nice sampling properties including consistency in estimation, rank selection, prediction, and model selection. Numerical studies on synthetic and real data sets show that SEED outperforms the state-of-the-art approaches for large-scale matrix estimation problem.},
archivePrefix = {arXiv},
arxivId = {1608.03686},
author = {Bahadori, Mohammad Taha and Zheng, Zemin and Liu, Yan and Lv, Jinchi},
eprint = {1608.03686},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/高维统计分析/Multi-Response/Scalable Interpretable Multi-Response Regression via Seed.pdf:pdf},
title = {{Scalable Interpretable Multi-Response Regression via SEED}},
url = {http://arxiv.org/abs/1608.03686},
year = {2016}
}
@article{Brabanter2013,
abstract = {We present a fully automated framework to estimate derivatives nonparametrically without estimating the regression function. Derivative estimation plays an important role in the exploration of structures in curves (jump detection and discontinuities), comparison of regression curves, analysis of human growth data, etc. Hence, the study of estimating derivatives is equally important as regression estimation itself. Via empirical derivatives we approximate the qth order derivative and create a new data set which can be smoothed by any nonparametric regression estimator. We derive L1 and L2 rates and establish consistency of the estimator. The new data sets created by this technique are no longer independent and identically distributed (i.i.d.) random variables anymore. As a consequence, automated model selection criteria (data-driven procedures) break down. Therefore, we propose a simple factor method, based on bimodal kernels, to effectively deal with correlated data in the local polynomial regression framework.},
author = {Brabanter, Kris De and Brabanter, J De},
doi = {10.1111/1467-9469.00056},
file = {::},
isbn = {1532-4435},
issn = {0303-6898},
journal = {Journal of Machine {\ldots}},
keywords = {empirical derivative,factor rule,model selection,nonparametric derivative estimation},
pages = {281--301},
title = {{Derivative Estimation with Local Polynomial Fitting}},
url = {http://jmlr.org/papers/volume14/debrabanter13a/debrabanter13a.pdf},
volume = {14},
year = {2013}
}
@article{chen2017regulation,
author = {Chen, Kun and Liu, Juan and Cao, Xuetao},
journal = {Journal of autoimmunity},
pages = {1--11},
publisher = {Elsevier},
title = {{Regulation of type I interferon signaling in immunity and inflammation: a comprehensive review}},
volume = {83},
year = {2017}
}
@article{Yu2012,
abstract = {Precise measurement of the forcing variable in regression discontinuity designs is critical to precise evaluation of treatment e¤ects. Such evaluation can be sensitive to measurement errors, which are preva-lent in many applications of regression discontinuity designs. The present paper studies identi{\ldots}cation of treatment e¤ects using local polynomial estimation in the presence of measurement error. The main {\ldots}ndings are as follows. In sharp designs, when the measurement error is {\ldots}xed, the treatment e¤ect can be identi{\ldots}ed in some special cases if the treatment is based on the contaminated forcing variable, but cannot be identi{\ldots}ed if the treatment is based on the error free forcing variable. If the measurement error is local to zero, the treatment e¤ect can be identi{\ldots}ed with a small extra bias and without e¢ ciency loss if the treatment is based on the contaminated forcing variable; the treatment e¤ect can be identi{\ldots}ed with e¢ ciency loss and a large bias if the treatment is based on the error free forcing variable and the treatment status can be observed; the treatment e¤ect cannot be identi{\ldots}ed if the treatment is based on the error free forcing variable and the treatment status cannot be observed unless the measurement error is extremely small. The results are extended to fuzzy designs. Monte Carlo results con{\ldots}rm the theoretical analysis.},
author = {Yu, Ping},
file = {::},
keywords = {C13,C14,C21,Essential Heterogeneity JEL-Classification,Identi{\ldots}ability,Local Polynomial Es-timator,Measurement Error,Regression Discontinuity Design},
title = {{Identification of Treatment Effects in Regression Discontinuity Designs with Measurement Error}},
year = {2012}
}
@inproceedings{ram2011density,
author = {Ram, Parikshit and Gray, Alexander G},
booktitle = {Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Density estimation trees.pdf:pdf},
organization = {ACM},
pages = {627--635},
title = {{Density estimation trees}},
year = {2011}
}
@article{Kallus2016a,
abstract = {We demonstrate the importance of structural priors for effective, efficient large-scale dynamic assortment personalization. Assortment personalization is the problem of choosing, for each individual or consumer segment (type), a best assortment of products, ads, or other offerings (items) so as to maximize revenue. This problem is central to revenue management in e-commerce, online advertising, and multi-location brick-and-mortar retail, where both items and types can number in the thousands-to-millions. Data efficiency is paramount in this large-scale setting. A good personalization strategy must dynamically balance the need to learn consumer preferences and to maximize revenue. We formulate the dynamic assortment personalization problem as a discrete-contextual bandit with {\$}m{\$} contexts (customer types) and many arms (assortments of the {\$}n{\$} items). We assume that each type's preferences follow a simple parametric model with {\$}n{\$} parameters. In all, there are {\$}mn{\$} parameters, and existing literature suggests that order optimal regret scales as {\$}mn{\$}. However, this figure is orders of magnitude larger than the data available in large-scale applications, and imposes unacceptably high regret. In this paper, we impose natural structure on the problem -- a small latent dimension, or low rank. In the static setting, we show that this model can be efficiently learned from surprisingly few interactions, using a time- and memory-efficient optimization algorithm that converges globally whenever the model is learnable. In the dynamic setting, we show that structure-aware dynamic assortment personalization can have regret that is an order of magnitude smaller than structure-ignorant approaches. We validate our theoretical results empirically.},
archivePrefix = {arXiv},
arxivId = {1610.05604},
author = {Kallus, Nathan and Udell, Madeleine},
eprint = {1610.05604},
file = {::;::},
keywords = {assortment planning,contextual bandit,discrete choice,high-dimensional,personalization},
title = {{Dynamic Assortment Personalization in High Dimensions}},
url = {http://arxiv.org/abs/1610.05604},
year = {2016}
}
@article{Beaulieu2012,
abstract = {Recently, there have been an increasing number of studies using change-point methods to detect artificial or natural discontinuities and regime shifts in climate. However, a major drawback with most of the currently used change-point methods is the lack of flexibility (able to detect one specific type of shift under the assumption that the residuals are independent). As temporal variations in climate are complex, it may be difficult to identify change points with very simple models. Moreover, climate time series are known to exhibit autocorrelation, which corresponds to a model misspecification if not taken into account and can lead to the detection of non-existent shifts. In this study, we extend a method known as the informational approach for change-point detection to take into account the presence of autocorrelation in the model. The usefulness and flexibility of this approach are demonstrated through applications. Furthermore, it is highly desirable to develop techniques that can detect shifts soon after they occur for climate monitoring. To address this, we also carried out a simulation study in order to investigate the number of years after which an abrupt shift is detectable. We use two decision rules in order to decide whether a shift is detected or not, which represents a trade-off between increasing our chances of detecting a shift and reducing the risk of detecting a shift while in reality there is none. We show that, as of now, we have good chances to detect an abrupt shift with a magnitude that is larger than that of the standard deviation in the series of observations. For shifts with a very large magnitude (three times the standard deviation), our simulation study shows that after only 4 years the probabilities of shift detection reach nearly 100 per cent. This reveals that the approach has potential for climate monitoring.},
author = {Beaulieu, C. and Chen, J. and Sarmiento, J. L.},
doi = {10.1098/rsta.2011.0383},
file = {::},
isbn = {1364-503X},
issn = {1364-503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {abrupt climate change,autocorrelation,change-point detection,regime shift},
number = {1962},
pages = {1228--1249},
pmid = {22291231},
title = {{Change-point analysis as a tool to detect abrupt climate variations}},
url = {http://rsta.royalsocietypublishing.org/cgi/doi/10.1098/rsta.2011.0383},
volume = {370},
year = {2012}
}
@article{Unknown,
author = {Unknown},
file = {::},
pages = {1--21},
title = {{Generalized Least Squares}},
volume = {2}
}
@misc{Package2016,
author = {Package, Type and Exact, Title and Segmentation, Bayesian and Cleynen, Author Alice},
file = {::},
title = {{Package ‘ EBS '}},
year = {2016}
}
@article{Wang2011,
author = {Wang, Lie and Brown, Lawrence D. and Cai, T. Tony},
doi = {10.1214/11-EJS621},
file = {::},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Asymptotic efficiency,Difference-based method,Kernel method,Partial linear model,Semiparametric model,Wavelet thresholding method},
pages = {619--641},
title = {{A difference based approach to the semiparametric partial linear model}},
volume = {5},
year = {2011}
}
@article{McElroy1967,
abstract = {Abstract It is shown that in a standard linear regression model ordinary least-squares estimators are best linear unbiased if and only if the errors have the same variance and the same nonnegative coefficient of correlation between each pair.},
author = {McElroy, F. W.},
doi = {10.1080/01621459.1967.10500935},
file = {::},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
number = {320},
pages = {1302--1304},
title = {{A Necessary and Sufficient Condition That Ordinary Least-Squares Estimators Be Best Linear Unbiased}},
volume = {62},
year = {1967}
}
@article{Hardle2001,
author = {H{\"{a}}rdle, Wolfgang and Mammen, Enno and Proenca, Isabel},
doi = {10.1080/02331880290013965},
file = {::},
isbn = {0233188010},
journal = {Statistics},
keywords = {Nonparametric change-point tests,Polynomial smoothing},
number = {May 2012},
pages = {37--41},
title = {{Change-Point Detection With NonParametric Regression}},
url = {http://dx.doi.org/10.1080/02331880210930},
year = {2001}
}
@article{Maik,
author = {Maik, D},
doi = {10.1007/978-0-8176-4971-5},
file = {::},
isbn = {9780817649715},
keywords = {and phrases,change-points,m-estimates,rate of consis-,regression},
pages = {207--221},
title = {{Change Point Estimation in Regression Models}}
}
@article{hall1990asymptotically,
author = {Hall, Peter and Kay, J W and Titterinton, D M},
journal = {Biometrika},
number = {3},
pages = {521--528},
publisher = {Oxford University Press},
title = {{Asymptotically optimal difference-based estimation of variance in nonparametric regression}},
volume = {77},
year = {1990}
}
@article{Truong2018,
abstract = {In this work, methods to detect one or several change points in multivariate time series are reviewed. They include retrospective (off-line) procedure such as maximum likelihood estimation, regression, kernel methods, etc. In this large area of research, applications are numerous and diverse; many different models and operational constraints (on precision, complexity,...) exist. A formal framework for change point detection is introduced to give sens to this significant body of work. Precisely, all methods are described as a collection of three elements: a cost function, a search method and a constraint on the number of changes to detect. For a given method, we detail the assumed signal model, the associated algorithm, theoretical guarantees (if any) and the application domain. This approach is intended to facilitate prototyping of change point detection methods: for a given segmentation task, one can appropriately choose among the described elements to design an algorithm.},
annote = {问题描述部分, 说明了, 位置下标与比例的关系},
archivePrefix = {arXiv},
arxivId = {1801.00718},
author = {Truong, Charles and Oudre, Laurent and Vayatis, Nicolas},
eprint = {1801.00718},
file = {::},
keywords = {change point detection,non-parametric method,review,signal segmentation},
title = {{A review of change point detection methods}},
url = {http://arxiv.org/abs/1801.00718},
year = {2018}
}
@article{mishra2017co-sparse,
author = {Mishra, Aditya and Dey, Dipak K and Chen, Kun},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/高维统计分析/Classical Papers/18.Co-Sparse.pdf:pdf},
journal = {Journal of Computational and Graphical Statistics},
number = {4},
pages = {814--825},
publisher = {Taylor {\&} Francis},
title = {{Sequential co-sparse factor regression}},
volume = {26},
year = {2017}
}
@article{Charnigo2011,
author = {Charnigo, Richard and Hall, Benjamin and Srinivasan, Cidambi},
doi = {10.1198/TECH.2011.09147},
file = {::},
issn = {0040-1706},
journal = {Technometrics},
keywords = {analytic chemistry,nonparametric regression,pattern recognition,raman spec-,troscopy,tuning parameter},
number = {3},
pages = {238--253},
title = {{A Generalized C p Criterion for Derivative Estimation}},
url = {http://www.tandfonline.com/doi/abs/10.1198/TECH.2011.09147},
volume = {53},
year = {2011}
}
@misc{Hall2005,
abstract = {It is shown that, for kernel-based classification with univariate distributions and two populations, optimal bandwidth choice has a dichotomous character. If the two densities cross at just one point, where their curvatures have the same signs, then minimum Bayes risk is achieved using bandwidths which are an order of magnitude larger than those which minimize pointwise estimation error. On the other hand, if the curvature signs are different, or if there are multiple crossing points, then bandwidths of conventional size are generally appropriate. The range of different modes of behavior is narrower in multivariate settings. There, the optimal size of bandwidth is generally the same as that which is appropriate for pointwise density estimation. These properties motivate empirical rules for bandwidth choice. {\textcopyright} Institute of Mathematical Statistics, 2005.},
author = {Hall, Peter and Kang, Kee Hoon},
booktitle = {Annals of Statistics},
doi = {10.1214/009053604000000959},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Bandwidth Choice for Nonparametric Regression{\_}Rice{\_}1984.pdf:pdf},
issn = {00905364},
keywords = {Bayes risk,Bootstrap,Classification error,Cross-validation,Discrimination,Error rate,Kernel methods,Nonparametric density estimation},
number = {1},
pages = {284--306},
title = {{Bandwidth choice for nonparametric classification}},
volume = {33},
year = {2005}
}
@article{HNNagaraja,
author = {{H N Nagaraja}, H A David},
file = {::},
title = {{Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals of Statistics. {\textregistered} www.jstor.org}}
}
@article{Nguyen2018,
abstract = {In this paper, we consider the problem of estimating a conditional density in moderately large dimensions. Much more informative than regression functions, conditional densities are of main interest in recent methods, particularly in the Bayesian framework (studying the posterior distribution, finding its modes...). Considering a recently studied family of kernel estimators, we select a pointwise multivariate bandwidth by revisiting the greedy algorithm Rodeo (Regularisation Of Derivative Expectation Operator). The method addresses several issues: being greedy and computationally efficient by an iterative procedure, avoiding the curse of high dimensionality under some suitably defined sparsity conditions by early variable selection during the procedure, converging at a quasi-optimal minimax rate.},
archivePrefix = {arXiv},
arxivId = {1801.06477},
author = {Nguyen, Minh-lien Jeanne},
eprint = {1801.06477},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Nonparametric method for space conditional density estimation in moderately large dimensions.pdf:pdf},
keywords = {conditional density,greedy algorithm,high dimension,kernel density estimators,minimax rates,nonparametric inference,sparsity},
month = {jan},
title = {{Nonparametric method for space conditional density estimation in moderately large dimensions}},
url = {http://arxiv.org/abs/1801.06477},
year = {2018}
}
@article{marron1992exact,
author = {Marron, J Steve and Wand, Matt P},
journal = {The Annals of Statistics},
pages = {712--736},
publisher = {JSTOR},
title = {{Exact mean integrated squared error}},
year = {1992}
}
@article{gasser1986residual,
author = {Gasser, Theo and Sroka, Lothar and Jennen-Steinmetz, Christine},
journal = {Biometrika},
number = {3},
pages = {625--633},
publisher = {Oxford University Press},
title = {{Residual variance and residual pattern in nonlinear regression}},
volume = {73},
year = {1986}
}
@unpublished{wu2019SESS,
address = {Hefei},
author = {Wu, Jie and Zheng, Zemin},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/高维统计分析/Multi-Response/SESS-WuJie.pdf:pdf},
institution = {USTC},
title = {{Multivariate Knockoff Filter for Dimension Reduction and Controlled Variable Selection in Multivariate Regression}},
year = {2019}
}
@article{Yu2016,
author = {Yu, Ping},
doi = {10.1080/07474938.2013.833831},
file = {::},
issn = {15324168},
journal = {Econometric Reviews},
keywords = {Instrumental variable estimator,Local polynomial estimator,Optimal rate of convergence,Partially linear estimator,Partially polynomial estimator,Regression discontinuity design},
number = {4},
pages = {586--637},
title = {{Understanding Estimators of Treatment Effects in Regression Discontinuity Designs}},
volume = {35},
year = {2016}
}
@article{yao1989least,
author = {Yao, Yi-Ching and Au, Siu-Tong},
journal = {Sankhy{\{}$\backslash$=a{\}}: The Indian Journal of Statistics, Series A},
pages = {370--381},
publisher = {JSTOR},
title = {{Least-squares estimation of a step function}},
year = {1989}
}
@article{Hornus2008,
author = {Hornus, Samuel and Boissonnat, Jean-daniel},
file = {::},
title = {{An efficient implementation of Delaunay triangulations in medium dimensions An efficient implementation of Delaunay triangulations in medium dimensions}},
year = {2008}
}
@article{escobar1995bayesian,
author = {Escobar, Michael D and West, Mike},
journal = {Journal of the american statistical association},
number = {430},
pages = {577--588},
publisher = {Taylor {\&} Francis},
title = {{Bayesian density estimation and inference using mixtures}},
volume = {90},
year = {1995}
}
@article{Boissonnat2010,
author = {Boissonnat, Jean-daniel},
file = {::},
number = {January},
title = {{Convex Hulls , Voronoi Diagrams and Delaunay Triangulations Convex hull}},
year = {2010}
}
@article{elgammal2003efficient,
author = {Elgammal, Ahmed and Duraiswami, Ramani and Davis, Larry S},
journal = {IEEE Transactions on Pattern Analysis {\&} Machine Intelligence},
number = {11},
pages = {1499--1504},
publisher = {IEEE},
title = {{Efficient kernel density estimation using the fast gauss transform with applications to color modeling and tracking}},
year = {2003}
}
@article{candes2018panning,
author = {Candes, Emmanuel and Fan, Yingying and Janson, Lucas and Lv, Jinchi},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
number = {3},
pages = {551--577},
publisher = {Wiley Online Library},
title = {{Panning for gold:‘model-X'knockoffs for high dimensional controlled variable selection}},
volume = {80},
year = {2018}
}
@article{yao1984estimation,
author = {Yao, Yi-Ching},
journal = {The Annals of Statistics},
pages = {1434--1447},
publisher = {JSTOR},
title = {{Estimation of a noisy discrete-time step function: Bayes and empirical Bayes approaches}},
year = {1984}
}
@article{Fan2010,
abstract = {High dimensional statistical problems arise from diverse fields of scientific research and technological development. Variable selection plays a pivotal role in contemporary statistical learning and scientific discoveries. The traditional idea of best subset selection methods, which can be regarded as a specific form of penalized likelihood, is computationally too expensive for many modern statistical applications. Other forms of penalized likelihood methods have been successfully developed over the last decade to cope with high dimensionality. They have been widely applied for simultaneously selecting important variables and estimating their effects in high dimensional statistical inference. In this article, we present a brief account of the recent developments of theory, methods, and implementations for high dimensional variable selection. What limits of the dimensionality such methods can handle, what the role of penalty functions is, and what the statistical properties are rapidly drive the advances of the field. The properties of non-concave penalized likelihood and its roles in high dimensional statistical modeling are emphasized. We also review some recent advances in ultra-high dimensional variable selection, with emphasis on independence screening and two-scale methods.},
archivePrefix = {arXiv},
arxivId = {0910.1122},
author = {Fan, Jianqing and Lv, Jinchi},
eprint = {0910.1122},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/A Selective Overview of Variable Selection in High Dimensional Feature Space.pdf:pdf},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {Dimensionality reduction,Folded-concave penalty,High dimensionality,LASSO,Model selection,Oracle property,Penalized least squares,Penalized likelihood,SCAD,Sure independence screening,Sure screening,Variable selection},
month = {oct},
number = {1},
pages = {101--148},
title = {{A Selective Overview of Variable Selection in High Dimensional Feature Space (Invited Review Article)}},
url = {http://arxiv.org/abs/0910.1122},
volume = {20},
year = {2009}
}
@article{Muller1997,
abstract = {We consider a fixed design regression model where the regression function is assumed to be smooth, i.e., Lipschitz continuous, except for a point where it has only one-sided limits and a local discontinuity occurs. We propose a two-step estimator for the location of this change point and study its asymptotic convergence properties. In a first step, initial pilot estimates of the change point and associated asymptotically shrinking intervals which contain the true change point with probability converging to 1 are obtained. In the second step, a weighted mean difference depending on the assumed location of the change point is maximized within these intervals and the maximizing argument is then the final change point estimator. It is shown that this estimator attains the rate Op(n-1) in the fixed jump case. In the contiguous case, the estimator attains the rate Op(n-1$\Delta$n)-2, where $\Delta$nis the sequence of jump sizes which in this case is assumed to converge to 0. For the contiguous case an invariance principle is established. A sequence of appropriately scaled deviation processes is shown to converge to a two-sided Brownian motion with triangular drift.},
author = {M{\"{u}}ller, Hans Georg and Song, Kai Sheng},
doi = {10.1016/S0167-7152(96)00197-6},
file = {::},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Asymptotics,Brownian motion,Discontinuity,Functional limit theorem,Nonparametric regression,Rate of convergence,Triangular drift,Weak convergence},
number = {4},
pages = {323--335},
title = {{Two-stage change-point estimators in smooth regression models}},
volume = {34},
year = {1997}
}
@article{Munk2005,
abstract = {We consider the problem of estimating the noise variance in homoscedastic nonparametric regression models. For low dimensional covariates t ∈ ℝd, d=1, 2, difference-based estimators have been investigated in a series of papers. For a given length of such an estimator, difference schemes which minimize the asymptotic mean-squared error can be computed for d=1 and d=2. However, from numerical studies it is known that for finite sample sizes the performance of these estimators may be deficient owing to a large finite sample bias. We provide theoretical support for these findings. In particular, we show that with increasing dimension d this becomes more drastic. If d4, these estimators even fail to be consistent. A different class of estimators is discussed which allow better control of the bias and remain consistent when d4. These estimators are compared numerically with kernel-type estimators (which are asymptotically efficient), and some guidance is given about when their use becomes necessary.},
author = {Munk, Axel and Bissantz, Nicolai and Wagner, Thorsten and Freitag, Gudrun},
doi = {10.1111/j.1467-9868.2005.00486.x},
file = {::},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {difference scheme,efficiency,imaging,local polynomial,nonparametric regression,polynomial weighting scheme,residual variance,variance estimation},
number = {1},
pages = {19--41},
title = {{On difference-based variance estimation in nonparametric regression when the covariate is high dimensional}},
volume = {67},
year = {2005}
}
@book{Goldstein1986,
author = {Goldstein, B Y H},
file = {::},
isbn = {0470866977},
title = {{Generalized Least Squares}},
volume = {7},
year = {1986}
}
@phdthesis{Liu2010,
author = {Liu, Han},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Liu{\_}Han{\_}PhD{\_}Thesis{\_}Nonparametric Learning in High Dimensions.pdf:pdf},
pages = {305},
school = {CMU},
title = {{Nonparametric Learning in High Dimensions.pdf}},
type = {Ph.D.},
year = {2010}
}
@article{Wang2014,
abstract = {Although Bayesian density estimation using discrete mixtures has good performance in modest dimensions, there is a lack of statistical and computational scalability to high-dimensional multivariate cases. To combat the curse of dimensionality, it is necessary to assume the data are concentrated near a lower-dimensional subspace. However, Bayesian methods for learning this subspace along with the density of the data scale poorly computationally. To solve this problem, we propose an empirical Bayes approach, which estimates a multiscale dictionary using geometric multiresolution analysis in a first stage. We use this dictionary within a multiscale mixture model, which allows uncertainty in component allocation, mixture weights and scaling factors over a binary tree. A computational algorithm is proposed, which scales efficiently to massive dimensional problems. We provide some theoretical support for this geometric density estimation (GEODE) method, and illustrate the performance through simulated and real data examples.},
archivePrefix = {arXiv},
arxivId = {1410.7692},
author = {Wang, Ye and Canale, Antonio and Dunson, David},
eprint = {1410.7692},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Scalable multiscale density estimation.pdf:pdf},
month = {oct},
title = {{Scalable multiscale density estimation}},
url = {http://arxiv.org/abs/1410.7692},
year = {2014}
}
@article{sun2012scaled,
author = {Sun, Tingni and Zhang, Cun-Hui},
journal = {Biometrika},
number = {4},
pages = {879--898},
publisher = {Oxford University Press},
title = {{Scaled sparse linear regression}},
volume = {99},
year = {2012}
}
@misc{Maciak2014,
author = {Maciak, Mat{\'{u}}{\v{s}}},
file = {::},
title = {{Change-point Estimation and Inference in Nonparametric Regression Using Different Regularization Concepts}},
year = {2014}
}
@misc{Liu2007b,
abstract = {We consider the problem of estimating the joint density of a d-dimensional random vector X = (X 1,X 2,...,X d) when d is large. We assume that the density is a product of a parametric component and a nonparametric component which depends on an unknown subset of the variables. Using a modification of a recently developed nonparametric regression framework called rodeo (regularization of derivative expectation operator), we propose a method to greedily select bandwidths in a kernel density estimate. It is shown empirically that the density rodeo works well even for very high dimensional problems. When the unknown density function satisfies a suitably defined sparsity condition, and the parametric baseline density is smooth, the approach is shown to achieve near optimal minimax rates of convergence, and thus avoids the curse of dimensionality.},
author = {Liu, Han and Lafferty, John and Wasserman, Larry},
booktitle = {Journal of Machine Learning Research},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Sparse Nonparametric Density Estimation in High Dimensions Using the Rodeo{\_}ed2.pdf:pdf},
issn = {15324435},
pages = {283--290},
title = {{Sparse nonparametric density estimation in high dimensions using the rodeo}},
volume = {2},
year = {2007}
}
@misc{Han2018,
abstract = {We consider the problem of estimating high-dimensional and nonparametric distributions in distributed networks, where each sensor in the network observes an independent sample from the underlying distribution and can communicate it to a central processor by writing at most k bits on a public blackboard. We obtain matching upper and lower bounds for the minimax risk of estimating the underlying distribution under L 1loss. Our results reveal that the minimax risk reduces exponentially in k. Instead of relying on strong data processing inequalities for the converse as commonly done in the literature, we build on a new representation of the communication constraint, which leads to a tight characterization of the problem.},
author = {Han, Yanjun and Mukherjee, Pritam and Ozgur, Ayfer and Weissman, Tsachy},
booktitle = {IEEE International Symposium on Information Theory - Proceedings},
doi = {10.1109/ISIT.2018.8437818},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Distributed Statistical Estimation of High-Dimensional and Nonparametric Distributions.pdf:pdf},
isbn = {9781538647806},
issn = {21578095},
pages = {506--510},
title = {{Distributed Statistical Estimation of High-Dimensional and Nonparametric Distributions}},
volume = {2018-June},
year = {2018}
}
@article{hjort1995nonparametric,
author = {Hjort, Nils Lid and Glad, Ingrid K},
journal = {The Annals of Statistics},
pages = {882--904},
publisher = {JSTOR},
title = {{Nonparametric density estimation with a parametric start}},
year = {1995}
}
@article{Raimondo2004,
abstract = {Newly available wavelet bases on multi-resolution analysis have exciting implications for detection of change-points. By checking the absolute value of wavelet coefficients one can detect discontinuities in an otherwise smooth curve even in the presence of additive noise. In this paper, we combine wavelet methods and extreme value theory to test the presence of an arbitrary number of discontinuities in an unknown function observed with noise. Our approach is based on a Peaks Over Threshold modelling of noisy wavelet transforms. Particular features of our method include the estimation of the extreme value index in the tail of the noise distribution. The critical region of our test is derived using a Generalised Pareto Distribution approximation to normalised sums. Asymptotic results show that our method is powerful in a wide range of medium size wavelet frequencies. We compare our test with competing approaches on simulated examples and illustrate the methodonDow-Jones data.},
author = {Raimondo, Marc and Tajvidi, Nader},
file = {::},
issn = {1017-0405},
journal = {Statistica Sinica},
keywords = {Change point,general Pareto distribution,nonparametric regression,peaks over threshold,tail exponent wavelets},
pages = {395--412},
title = {{A peaks over threshold model for change-point detection by wavelets}},
volume = {14},
year = {2004}
}
@article{stone1990large,
author = {Stone, Charles J},
journal = {The Annals of Statistics},
pages = {717--741},
publisher = {JSTOR},
title = {{Large-sample inference for log-spline models}},
year = {1990}
}
@article{Dai2017,
abstract = {{\textcopyright} Institute of Mathematical Statistics, 2017. Difference-based methods do not require estimating the mean function in nonparametric regression and are therefore popular in practice. In this paper, we propose a unified framework for variance estimation that combines the linear regression method with the higher-order difference estimators systematically. The unified framework has greatly enriched the existing literature on variance estimation that includes most existing estimators as special cases. More importantly, the unified framework has also provided a smart way to solve the challenging difference sequence selection problem that remains a long-standing controversial issue in nonparametric regression for several decades. Using both theory and simulations, we recommend to use the ordinary difference sequence in the unified framework, no matter if the sample size is small or if the signal-to-noise ratio is large. Finally, to cater for the demands of the application, we have developed a unified R package, named VarED, that integrates the existing difference-based estimators and the unified estimators in nonparametric regression and have made it freely available in the R statistical program http://cran.r-project.org/web/packages/.},
author = {Dai, Wenlin and Tong, Tiejun and Zhu, Lixing},
doi = {10.1214/17-STS613},
file = {::},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Difference-based estimator, nonparametric regressi,and phrases,difference-based estimator,nonparametric regres-,optimal difference sequence,ordinary difference sequence,residual,sion},
number = {3},
pages = {455--468},
title = {{On the Choice of Difference Sequence in a Unified Framework for Variance Estimation in Nonparametric Regression}},
url = {https://projecteuclid.org/euclid.ss/1504253126},
volume = {32},
year = {2017}
}
@article{Hanushek1977,
author = {Hanushek, Eric A. and Jackson, John E.},
file = {::},
isbn = {0470866977},
journal = {Statistical Methods for Social Scientists},
pages = {141--178},
title = {{Generalized Least Squares}},
year = {1977}
}
@article{Sharifzadeh2005,
abstract = {In this paper, we propose a novel approach to address the problem of change detection in time series data. Our approach is based on wavelet footprints proposed originally by the signal processing community for signal compression. We, however, exploit the properties of footprints to capture discontinuities in a signal. We show that transforming data using footprints generates nonzero coefficients only at the change points. Exploiting this property, we propose a change detection query processing scheme which employs footprint-transformed data to identify change points, their amplitudes, and degrees of change efficiently and accurately. Our analytical and empirical results show that our approach outperforms the best known change detection approach in terms of both performance and accuracy. Furthermore, unlike the state of the art approaches, our query response time is independent of the number of change points and the user-defined change threshold.},
author = {Sharifzadeh, Mehdi and Azmoodeh, Farnaz and Shahabi, Cyrus},
doi = {10.1007/11535331_8},
file = {::},
isbn = {978-3-540-28127-6},
journal = {Advances in Spatial and Temporal Databases, Lecture Notes in Computer Science Volume 3633},
pages = {127--144},
title = {{Change detection in time series data using wavelet footprints}},
url = {http://link.springer.com/chapter/10.1007/11535331{\_}8},
year = {2005}
}
@incollection{DasGupta2012,
author = {DasGupta, Anirban and Lahiri, S.N.},
doi = {10.1214/11-IMSCOLL801},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Density estimation in high and ultra high dimensions, regularization, and the L1 asymptotics.pdf:pdf},
pages = {1--23},
title = {{Density estimation in high and ultra high dimensions, regularization, and the L 1 asymptotics}},
url = {http://projecteuclid.org/euclid.imsc/1331731608},
year = {2012}
}
@article{krishnaiah198819,
author = {Krishnaiah, P R and Miao, B Q},
journal = {Handbook of statistics},
pages = {375--402},
publisher = {Elsevier},
title = {{19 Review about estimation of change points}},
volume = {7},
year = {1988}
}
@article{Loader1996,
author = {Loader, Clive R},
doi = {10.1214/aos/1032298290},
file = {::},
isbn = {0090-5364},
issn = {00905364},
journal = {The Annals of Statistics},
keywords = {Boundary crossing,change point,nonparametric regression},
number = {4},
pages = {1667--1678},
title = {{Change point estimation using nonparametric regression}},
url = {http://projecteuclid.org/euclid.aos/1032298290},
volume = {24},
year = {1996}
}
@article{rosenblatt1956remarks,
author = {Rosenblatt, Murray},
journal = {The Annals of Mathematical Statistics},
pages = {832--837},
publisher = {JSTOR},
title = {{Remarks on some nonparametric estimates of a density function}},
year = {1956}
}
@article{sain1996locally,
author = {Sain, Stephan R and Scott, David W},
journal = {Journal of the American Statistical Association},
number = {436},
pages = {1525--1534},
publisher = {Taylor {\&} Francis Group},
title = {{On locally adaptive density estimation}},
volume = {91},
year = {1996}
}
@misc{Lafferty2005,
abstract = {We present a method for nonparametric regression that performs bandwidth selection and variable selection simultaneously. The approach is based on the technique of incrementally decreasing the bandwidth in directions where the gradient of the estimator with respect to bandwidth is large. When the unknown function satisfies a sparsity condition, our approach avoids the curse of dimensionality, achieving the optimal minimax rate of convergence, up to logarithmic factors, as if the relevant variables were known in advance. The method-called rodeo (regularization of derivative expectation operator)-conducts a sequence of hypothesis tests, and is easy to implement. A modified version that replaces hard with soft thresholding effectively solves a sequence of lasso problems.},
archivePrefix = {arXiv},
arxivId = {math/0506342},
author = {Lafferty, John and Wasserman, Larry},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {0506342},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Rodeo{\_}Sparse Nonparametric Regression in High Dimensions.pdf:pdf},
isbn = {9780262232531},
issn = {10495258},
pages = {707--714},
primaryClass = {math},
title = {{Rodeo: Sparse nonparametric regression in high dimensions}},
year = {2005}
}
@article{Wang2017,
abstract = {The existing differenced estimators of error variance in nonparametric regression are interpreted as kernel estimators, and some requirements for a “good” estimator of error variance are specified. A new differenced method is then proposed that estimates the errors as the intercepts in a sequence of simple linear regressions and constructs a variance estimator based on estimated errors. The new estimator satisfies the requirements for a “good” estimator and achieves the asymptotically optimal mean square error. A feasible difference order is also derived, which makes the estimator more applicable. To improve the finite-sample performance, two bias-corrected versions are further proposed. All three estimators are equivalent to some local polynomial estimators and thus can be interpreted as kernel estimators. To determine which of the three estimators to be used in practice, a rule of thumb is provided by analysis of the mean square error, which solves an open problem in error variance estimation which difference sequence to be used in finite samples. Simulation studies and a real data application corroborate the theoretical results and illustrate the advantages of the new method compared with the existing methods.},
author = {Wang, Wen Wu and Yu, Ping},
doi = {10.1016/j.csda.2016.07.012},
file = {::},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Bias correction,Difference order,Error estimation,Kernel estimation,Optimal difference sequence,Quadratic form,Taylor expansion},
pages = {125--143},
publisher = {Elsevier B.V.},
title = {{Asymptotically optimal differenced estimators of error variance in nonparametric regression}},
url = {http://dx.doi.org/10.1016/j.csda.2016.07.012},
volume = {105},
year = {2017}
}
@article{zheng2019scalable,
author = {Zheng, Zemin and Bahadori, M Taha and Liu, Yan and Lv, Jinchi},
journal = {Journal of Machine Learning Research},
number = {107},
pages = {1--34},
title = {{Scalable Interpretable Multi-Response Regression via SEED}},
volume = {20},
year = {2019}
}
@article{Jensen1997,
author = {Jensen, Uwe and L{\"{u}}tkebohmert, Costanze},
doi = {10.1002/9780470061572.eqr076},
file = {::},
isbn = {978-0-470-01861-3},
journal = {Encyclopedia of Statistics in Quality and Reliability},
keywords = {change-point,detection problem,hazard rate models,optimal stopping,regression models,sequentially observed data},
pages = {1--14},
title = {{Change-Point Models}},
url = {http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470018615.html},
year = {1997}
}
@article{silverman1982algorithm,
author = {Silverman, Bernhard W},
journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
number = {1},
pages = {93--99},
publisher = {JSTOR},
title = {{Algorithm AS 176: Kernel density estimation using the fast Fourier transform}},
volume = {31},
year = {1982}
}
@article{zheng2019scalable,
abstract = {Sparse reduced-rank regression is an important tool to uncover meaningful dependence structure between large numbers of predictors and responses in many big data applications such as genome-wide association studies and social media analysis. Despite the recent theoretical and algorithmic advances, scalable estimation of sparse reduced-rank regression remains largely unexplored. In this paper, we suggest a scalable procedure called sequential estimation with eigen-decomposition (SEED) which needs only a single top-{\$}r{\$} singular value decomposition to find the optimal low-rank and sparse matrix by solving a sparse generalized eigenvalue problem. Our suggested method is not only scalable but also performs simultaneous dimensionality reduction and variable selection. Under some mild regularity conditions, we show that SEED enjoys nice sampling properties including consistency in estimation, rank selection, prediction, and model selection. Numerical studies on synthetic and real data sets show that SEED outperforms the state-of-the-art approaches for large-scale matrix estimation problem.},
archivePrefix = {arXiv},
arxivId = {1608.03686},
author = {Zheng, Zemin and Bahadori, Mohammad Taha and Liu, Yan and Lv, Jinchi},
eprint = {1608.03686},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/高维统计分析/Multi-Response/Scalable Interpretable Multi-Response Regression via Seed.pdf:pdf},
journal = {Journal of Machine Learning Research},
number = {107},
pages = {1--34},
title = {{Scalable Interpretable Multi-Response Regression via SEED}},
url = {http://jmlr.org/papers/volume20/18-200/18-200.pdf http://arxiv.org/abs/1608.03686},
volume = {20},
year = {2019}
}
@article{heinrichs1995patterns,
author = {Heinrichs, Claudine and Munson, Peter J and Counts, Debra R and Cutler, Gordon B and Baron, Jeffrey and Lampl, Michelle and Cameron, No{\"{e}}l and Veldhuis, Johannes D and Johnson, Michael L},
journal = {Science},
number = {5209},
pages = {442--447},
publisher = {JSTOR},
title = {{Patterns of human growth}},
volume = {268},
year = {1995}
}
@article{Kwon2005,
author = {Kwon, D W},
file = {::},
keywords = {change point detection,network traffic,statistical hypothesis testing},
number = {979},
pages = {1--32},
title = {{Wavelet methods for the detection of anomalies and their application to network traffic analysis}},
year = {2005}
}
@book{csorgo1997limit,
author = {Cs{\"{o}}rg{\"{o}}, Mikl{\'{o}}s and Horv{\'{a}}th, Lajos},
publisher = {John Wiley {\&} Sons Inc},
title = {{Limit theorems in change-point analysis}},
volume = {18},
year = {1997}
}
@article{Deutsch1955,
abstract = {Several modifications of the Asch experiment in which the S judges the length of lines in the company of a group of "stooges" who carry out the experimenter's instructions are described. These include a face-to-face situation, an anonymous situation, and a group situation, with self-commitment, public commitment and Magic Pad commitment variations. The results indicate that, even when normative social influence in the direction of an incorrect judgment is largely removed (as in the anonymous situation), more errors are made by Ss in experimental groups than by Ss making their judgments when alone. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
address = {US},
author = {Deutsch, Morton and Gerard, Harold B},
doi = {10.1037/h0046408},
issn = {0096-851X(Print)},
journal = {The Journal of Abnormal and Social Psychology},
keywords = {*Commitment,*Judgment,*Self-Concept,Social Influences},
number = {3},
pages = {629--636},
publisher = {American Psychological Association},
title = {{A study of normative and informational social influences upon individual judgment.}},
volume = {51},
year = {1955}
}
@article{Robinson2010,
author = {Robinson, Author P M},
file = {::},
journal = {Society},
number = {4},
pages = {931--954},
title = {{MP-valued Z , and are column vectors and the prime indicates transposition .}},
volume = {56},
year = {2010}
}
@misc{Liu2007a,
abstract = {We consider the problem of estimating the joint density of a d-dimensional random vector X = (X 1,X 2,...,X d) when d is large. We assume that the density is a product of a parametric component and a nonparametric component which depends on an unknown subset of the variables. Using a modification of a recently developed nonparametric regression framework called rodeo (regularization of derivative expectation operator), we propose a method to greedily select bandwidths in a kernel density estimate. It is shown empirically that the density rodeo works well even for very high dimensional problems. When the unknown density function satisfies a suitably defined sparsity condition, and the parametric baseline density is smooth, the approach is shown to achieve near optimal minimax rates of convergence, and thus avoids the curse of dimensionality.},
author = {Liu, Han and Lafferty, John and Wasserman, Larry},
booktitle = {Journal of Machine Learning Research},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Sparse Nonparametric Density Estimation in High Dimensions Using the Rodeo{\_}detial.pdf:pdf},
issn = {15324435},
pages = {283--290},
title = {{Sparse nonparametric density estimation in high dimensions using the rodeo}},
volume = {2},
year = {2007}
}
@article{Brown2007,
abstract = {Consider a Gaussian nonparametric regression problem having both an unknown mean function and unknown variance function. This article presents a class of difference-based kernel estimators for the variance function. Optimal convergence rates that are uniform over broad functional classes and bandwidths are fully characterized, and asymptotic normality is also established. We also show that for suitable asymptotic formulations our estimators achieve the minimax rate.},
annote = {估计的是十分一般的回归方程的方差函数, 结果很有意义.
可以学习的是其中的渐进性质的部分, 可以用于我自己的论文的证明.(2018.5.5)},
archivePrefix = {arXiv},
arxivId = {0712.0898},
author = {Brown, Lawrence D. and Levine, M.},
doi = {10.1214/009053607000000145},
eprint = {0712.0898},
file = {::},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Asymptotic minimaxity,Nonparametric regression,Variance estimation},
month = {dec},
number = {5},
pages = {2219--2232},
title = {{Variance estimation in nonparametric regression via the difference sequence method}},
url = {http://arxiv.org/abs/0712.0898 http://dx.doi.org/10.1214/009053607000000145},
volume = {35},
year = {2007}
}
@article{staniswalis1989local,
author = {Staniswalis, Joan G},
journal = {Journal of the American Statistical Association},
number = {405},
pages = {284--288},
publisher = {Taylor {\&} Francis},
title = {{Local bandwidth selection for kernel estimates}},
volume = {84},
year = {1989}
}
@book{Wasserman2006All,
author = {Wasserman, Larry},
title = {{All of Nonparametric Statistics}},
year = {2006}
}
@article{csorgHo198820,
author = {Cs{\"{o}}rg$\backslash$Ho, Mikl{\'{o}}s and Horvath, Lajos},
journal = {Handbook of statistics},
pages = {403--425},
publisher = {Elsevier},
title = {{20 Nonparametric methods for changepoint problems}},
volume = {7},
year = {1988}
}
@article{Tan2009,
abstract = {本文由两部分构成．第一部分简要概述了变点问题的应用背景及其统计分析的研究 进展． 第二部分研究了至多一个变点时r分布的变点的检验和估计． 我们采用局部比较法，借助Gauss过程理论和滑窗方法，利用第一型极值分布逼近本文提出的统计量的分布，给出了检测变点岛的程序和估计，并应用Matlab进行模拟。由于r一分布簇在金 融领域有着十分重要的作用，因而这个课题的研究不仅具有理论方面研究的价值，同时 还具有一定的应用价值． 本文第一章里我们简要概述了变点问题的发展、研究状况、主要研究方法以及国内 学者在变点统计分析领域的研究． 在本文的第二章，我们研究了r分布至多一个交点的检验和统计推断，包括， 1：不 论A是否变化，r至多一个变点的检验；以及若变点存在，变点幻的估计． 2：不论一 是否变化，{\^{}}至多一个变点的检验． 第三章讨论了v不变时，lambda至多一个变点的另一检验方法． 本文的最后一章里，我们主要用Matlab对第二章、第三章中提出的统计量进行模拟．},
author = {Tan, Changchun},
file = {::},
pages = {32},
title = {{Gamma分布中至多一个变点的统计推断}},
year = {2009}
}
@article{Wang2019,
abstract = {Density Estimation is one of the central areas of statistics whose purpose is to estimate the probability density function underlying the observed data. It serves as a building block for many tasks in statistical inference, visualization, and machine learning. Density Estimation is widely adopted in the domain of unsupervised learning especially for the application of clustering. As big data become pervasive in almost every area of data sciences, analyzing high-dimensional data that have many features and variables appears to be a major focus in both academia and industry. High-dimensional data pose challenges not only from the theoretical aspects of statistical inference, but also from the algorithmic/computational considerations of machine learning and data analytics. This paper reviews a collection of selected nonparametric density estimation algorithms for high-dimensional data, some of them are recently published and provide interesting mathematical insights. The important application domain of nonparametric density estimation, such as {\{} modal clustering{\}}, are also included in this paper. Several research directions related to density estimation and high-dimensional data analysis are suggested by the authors.},
archivePrefix = {arXiv},
arxivId = {1904.00176},
author = {Wang, Zhipeng and Scott, David W.},
doi = {10.1002/wics.1461},
eprint = {1904.00176},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Nonparametric Density Estimation for High-Dimensional Data - Algorithms and Applications.pdf:pdf},
month = {mar},
title = {{Nonparametric Density Estimation for High-Dimensional Data - Algorithms and Applications}},
url = {http://arxiv.org/abs/1904.00176 http://dx.doi.org/10.1002/wics.1461},
year = {2019}
}
@article{Tecuapetla-Gomez2017b,
abstract = {We discuss a class of difference-based estimators for the autocovariance in nonparametric regression when the signal is discontinuous (change-point regression), possibly highly fluctuating, and the errors form a stationary {\$}m{\$}-dependent process. These estimators circumvent the explicit pre-estimation of the unknown regression function, a task which is particularly challenging for such signals. We provide explicit expressions for their mean squared errors when the signal function is piecewise constant (segment regression) and the errors are Gaussian. Based on this we derive biased-optimized estimates which do not depend on the particular (unknown) autocovariance structure. Notably, for positively correlated errors, that part of the variance of our estimators which depends on the signal is minimal as well. Further, we provide sufficient conditions for {\$}\backslashsqrt{\{}n{\}}{\$}-consistency; this result is extended to piecewise Holder regression with non-Gaussian errors. We combine our biased-optimized autocovariance estimates with a projection-based approach and derive covariance matrix estimates, a method which is of independent interest. Several simulation studies as well as an application to biophysical measurements complement this paper.},
archivePrefix = {arXiv},
arxivId = {1507.02485},
author = {Tecuapetla-G{\'{o}}mez, Inder and Munk, Axel},
doi = {10.1111/sjos.12256},
eprint = {1507.02485},
file = {::},
issn = {14679469},
journal = {Scandinavian Journal of Statistics},
keywords = {autocovariance estimation,change-point,convex projection,covariance matrix estimation,difference-based methods,discontinuous signal,m-dependent processes,mean squared error,non-parametric regression},
number = {2},
pages = {346--368},
title = {{Autocovariance Estimation in Regression with a Discontinuous Signal and m-Dependent Errors: A Difference-Based Approach}},
volume = {44},
year = {2017}
}
@article{bowman1984alternative,
author = {Bowman, Adrian W},
journal = {Biometrika},
number = {2},
pages = {353--360},
publisher = {Oxford University Press},
title = {{An alternative method of cross-validation for the smoothing of density estimates}},
volume = {71},
year = {1984}
}
@article{Robinson1988Root,
author = {Robinson, P M},
journal = {Econometrica},
number = {4},
pages = {931--954},
title = {{Root-N-Consistent Semiparametric Regression}},
volume = {56},
year = {1988}
}
@article{cleveland1988locally,
author = {Cleveland, William S and Devlin, Susan J},
journal = {Journal of the American statistical association},
number = {403},
pages = {596--610},
publisher = {Taylor {\&} Francis Group},
title = {{Locally weighted regression: an approach to regression analysis by local fitting}},
volume = {83},
year = {1988}
}
@article{Khodadadi2008,
author = {Khodadadi, Ahmad and Asgharian, Masoud},
file = {::},
pages = {1--236},
title = {{Collection of Biostatistics Research Archive Change-point Problem and Regression : An Annotated Bibliography Change-point Problem and Regression : An Annotated Bibliography}},
url = {http://biostats.bepress.com/cgi/viewcontent.cgi?article=1075{\&}context=cobra},
year = {2008}
}
@article{parzen1962estimation,
author = {Parzen, Emanuel},
journal = {The annals of mathematical statistics},
number = {3},
pages = {1065--1076},
publisher = {JSTOR},
title = {{On estimation of a probability density function and mode}},
volume = {33},
year = {1962}
}
@inproceedings{wasserman2006rodeo,
author = {Wasserman, Larry and Lafferty, John D},
booktitle = {Advances in Neural Information Processing Systems},
pages = {707--714},
title = {{Rodeo: Sparse nonparametric regression in high dimensions}},
year = {2006}
}
@article{Ruano2015,
abstract = {The accuracy of classification and regression tasks based on data driven models, such as Neural Networks or Support Vector Machines, relies to a good extent on selecting proper data for designing these models that covers the whole input ranges in which they will be employed. The convex hull algorithm is applied as a method for data selection; however the use of conventional implementations of this method in high dimensions, due to its high complexity, is not feasible. In this paper, we propose a randomized approximation convex hull algorithm which can be used for high dimensions in an acceptable execution time.},
author = {Ruano, Antonio and Khosravani, Hamid Reza and Ferreira, Pedro M.},
doi = {10.1016/j.ifacol.2015.08.119},
file = {::},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {Classification,Convex hull,Data selection problem,Neural networks,Regression,Support vector machines},
number = {10},
pages = {123--128},
publisher = {Elsevier Ltd.},
title = {{A randomized approximation convex hull algorith m for high dimensions}},
url = {http://dx.doi.org/10.1016/j.ifacol.2015.08.119},
volume = {28},
year = {2015}
}
@book{basseville1993detection,
author = {Basseville, Mich{\`{e}}le and Nikiforov, Igor V and Others},
publisher = {Prentice Hall Englewood Cliffs},
title = {{Detection of abrupt changes: theory and application}},
volume = {104},
year = {1993}
}
@article{Dai2016,
abstract = {We propose a simple framework for estimating derivatives without fitting the regression function in nonparametric regression. Unlike most existing methods that use the symmet-ric difference quotients, our method is constructed as a linear combination of observations. It is hence very flexible and applicable to both interior and boundary points, including most existing methods as special cases of ours. Within this framework, we define the variance-minimizing estimators for any order derivative of the regression function with a fixed bias-reduction level. For the equidistant design, we derive the asymptotic variance and bias of these estimators. We also show that our new method will, for the first time, achieve the asymptotically optimal convergence rate for difference-based estimators. Fi-nally, we provide an effective criterion for selection of tuning parameters and demonstrate the usefulness of the proposed method through extensive simulation studies of the first-and second-order derivative estimators.},
author = {Dai, Wenlin and Tong, Tiejun and Genton, Marc G},
file = {::},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Linear combination,Nonparametric derivative estimation,Nonparametric regression,Optimal sequence,Taylor expansion},
pages = {1--25},
title = {{Optimal Estimation of Derivatives in Nonparametric Regression}},
url = {http://www.jmlr.org/papers/volume17/15-640/15-640.pdf},
volume = {17},
year = {2016}
}
@article{silverman1982estimation,
author = {Silverman, Bernard W},
journal = {The Annals of Statistics},
pages = {795--810},
publisher = {JSTOR},
title = {{On the estimation of a probability density function by the maximum penalized likelihood method}},
year = {1982}
}
@article{Wang2015,
abstract = {A new method is proposed for estimating derivatives of a nonparametric regression function. By applying Taylor expansion technique to a derived symmetric diff erence sequence, we obtain a sequence of approximate linear regression representation in which the derivative is just the intercept term. Using locally weighted least squares, we estimate the derivative in the linear regression model. The estimator has less bias in both valleys and peaks of the true derivative function. For the special case of a domain with equispaced design points, the asymptotic bias and variance are derived; consistency and asymptotic normality are established. In simulations our estimators have less bias and mean square error than its main competitors, especially second order derivative estimator. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
author = {Wang, Wenwu and Lin, Lu},
file = {::},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Taylor expansion,bias-correction,locally weighted least squares,nonparametric derivative estimation,symmetric dierence sequence},
pages = {2617--2641},
title = {{Derivative Estimation Based on Difference Sequence via Locally Weighted Least Squares Regression}},
volume = {16},
year = {2015}
}
@book{snir1998mpi,
author = {Snir, Marc and Gropp, William and Otto, Steve and Huss-Lederman, Steven and Dongarra, Jack and Walker, David},
publisher = {MIT press},
title = {{MPI--the Complete Reference: The MPI core}},
volume = {1},
year = {1998}
}
@article{Yehong2018,
author = {Yehong, Liu and Guosheng, Yin},
file = {::},
keywords = {boosting,curvature regularization,ensemble learning,random crystal,target regularization},
pages = {47},
title = {{The Delaunay Triangulation Learner}},
year = {2018}
}
@book{YuZhuoxi2014,
author = {于卓熙},
publisher = {清华大学出版社},
title = {相依误差下部分线性模型的统计推断},
year = {2014}
}
@article{Tong2008,
abstract = {Difference-based estimators for the error variance are popular since they do not require the estimation of the mean function. Unlike most existing difference-based estimators, new estimators proposed by M{\"{u}}ller et al. (2003) and Tong and Wang (2005) achieved the asymptotic optimal rate as residual-based estimators. In this article, we study the relative errors of these difference-based estimators which lead to better understanding of the differences between them and residual-based estimators. To compute the relative error of the covariate-matched U-statistic estimator proposed by M{\"{u}}ller et al. (2003), we develop a modified version by using simpler weights. We further investigate its asymptotic property for both equidistant and random designs and show that our modified estimator is asymptotically efficient.},
author = {Tong, Tiejun and Liu, Anna and Wang, Yuedong},
doi = {10.1080/03610920802162656},
file = {::},
issn = {03610926},
journal = {Communications in Statistics - Theory and Methods},
keywords = {Asymptotically efficient,Bandwidth,Kernel estimator,Mean squared error,Nonparametric regression,Variance estimation},
number = {18},
pages = {2890--2902},
title = {{Relative errors of difference-based variance estimators in nonparametric regression}},
volume = {37},
year = {2008}
}
@article{lampl1992saltation,
author = {Lampl, Manfred and Veldhuis, Johannes D and Johnson, Mark L},
journal = {Science},
number = {5083},
pages = {801--803},
publisher = {American Association for the Advancement of Science},
title = {{Saltation and stasis: a model of human growth}},
volume = {258},
year = {1992}
}
@article{Dai2014a,
abstract = {Variance estimation is an important topic in nonparametric regression. In this paper, we propose a pairwise regression method for estimating the residual variance. Specifically, we regress the squared difference between observations on the squared distance between design points, and then estimate the residual variance as the intercept. Unlike most existing difference-based estimators that require a smooth regression function, our method applies to regression models with jump discontinuities. Our method also applies to the situations where the design points are unequally spaced. Finally, we conduct extensive simulation studies to evaluate the finite-sample performance of the proposed method and compare it with some existing competitors. {\textcopyright} 2013 {\textcopyright} 2013 Taylor  {\&}  Francis.},
author = {Dai, Wenlin and Tong, Tiejun},
doi = {10.1080/02664763.2013.842962},
file = {::},
issn = {02664763},
journal = {Journal of Applied Statistics},
keywords = {difference-based estimator,jump point,non-uniform design,nonparametric regression,pairwise regression,residual variance},
number = {3},
pages = {530--545},
title = {{Variance estimation in nonparametric regression with jump discontinuities}},
url = {http://dx.doi.org/10.1080/02664763.2013.842962},
volume = {41},
year = {2014}
}
@article{Tecuapetla-Gomez2017a,
abstract = {We discuss a class of difference-based estimators for the autocovariance in nonparametric regression when the signal is discontinuous (change-point regression), possibly highly fluctuating, and the errors form a stationary {\$}m{\$}-dependent process. These estimators circumvent the explicit pre-estimation of the unknown regression function, a task which is particularly challenging for such signals. We provide explicit expressions for their mean squared errors when the signal function is piecewise constant (segment regression) and the errors are Gaussian. Based on this we derive biased-optimized estimates which do not depend on the particular (unknown) autocovariance structure. Notably, for positively correlated errors, that part of the variance of our estimators which depends on the signal is minimal as well. Further, we provide sufficient conditions for {\$}\backslashsqrt{\{}n{\}}{\$}-consistency; this result is extended to piecewise Holder regression with non-Gaussian errors. We combine our biased-optimized autocovariance estimates with a projection-based approach and derive covariance matrix estimates, a method which is of independent interest. Several simulation studies as well as an application to biophysical measurements complement this paper.},
archivePrefix = {arXiv},
arxivId = {1507.02485},
author = {Tecuapetla-G{\'{o}}mez, Inder and Munk, Axel},
doi = {10.1111/sjos.12256},
eprint = {1507.02485},
file = {::},
issn = {14679469},
journal = {Scandinavian Journal of Statistics},
keywords = {autocovariance estimation,change-point,convex projection,covariance matrix estimation,difference-based methods,discontinuous signal,m-dependent processes,mean squared error,non-parametric regression},
number = {2},
pages = {346--368},
title = {{Autocovariance Estimation in Regression with a Discontinuous Signal and m-Dependent Errors: A Difference-Based Approach}},
volume = {44},
year = {2017}
}
@article{terrell1992variable,
author = {Terrell, George R and Scott, David W and Others},
journal = {The Annals of Statistics},
number = {3},
pages = {1236--1265},
publisher = {Institute of Mathematical Statistics},
title = {{Variable kernel density estimation}},
volume = {20},
year = {1992}
}
@article{racine2002parallel,
author = {Racine, Jeff},
journal = {Computational Statistics {\&} Data Analysis},
number = {2},
pages = {293--302},
publisher = {Elsevier},
title = {{Parallel distributed kernel estimation}},
volume = {40},
year = {2002}
}
@misc{Liu2007,
abstract = {We consider the problem of estimating the joint density of a d-dimensional random vector X = (X 1,X 2,...,X d) when d is large. We assume that the density is a product of a parametric component and a nonparametric component which depends on an unknown subset of the variables. Using a modification of a recently developed nonparametric regression framework called rodeo (regularization of derivative expectation operator), we propose a method to greedily select bandwidths in a kernel density estimate. It is shown empirically that the density rodeo works well even for very high dimensional problems. When the unknown density function satisfies a suitably defined sparsity condition, and the parametric baseline density is smooth, the approach is shown to achieve near optimal minimax rates of convergence, and thus avoids the curse of dimensionality.},
author = {Liu, Han and Lafferty, John and Wasserman, Larry},
booktitle = {Journal of Machine Learning Research},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Sparse Nonparametric Density Estimation in High Dimensions Using the Rodeo.pdf:pdf},
issn = {15324435},
pages = {283--290},
title = {{Sparse nonparametric density estimation in high dimensions using the rodeo}},
volume = {2},
year = {2007}
}
@inproceedings{Darkhovski1994,
abstract = {A general approach to change-point problems is proposed. This approach is based upon two ideas. The first idea is that any change-point problem can be reduced to the problem of detection of changes in the mean value of some new sequences. The second idea is that the nonparametric family of Kolmogorov- Smirnov type statistics can be used for change-point detection in these sequences. This general approach is implemented in two cases: (a) the problem of gradual change-point detection, and (b) change-point detection in two-phase regression model.},
author = {Darkhovski, B Y Boris S},
booktitle = {Change Point Problems},
doi = {10.1214/lnms/1215463117},
file = {::},
issn = {0749-2170},
keywords = {dependence,estimation,gradual change,kolmogorov-smirnov,mixing,optimality,two-phase regression},
number = {1993},
pages = {99--107},
title = {{Non parametric Methods in Change-point problems: a general approach and some concret algorithms}},
volume = {23},
year = {1994}
}
@article{rudemo1982empirical,
author = {Rudemo, Mats},
journal = {Scandinavian Journal of Statistics},
pages = {65--78},
publisher = {JSTOR},
title = {{Empirical choice of histograms and kernel density estimators}},
year = {1982}
}
@article{barber2015knockoffs,
author = {Barber, Rina Foygel and Cand{\`{e}}s, Emmanuel J},
file = {::},
journal = {The Annals of Statistics},
number = {5},
pages = {2055--2085},
publisher = {Institute of Mathematical Statistics},
title = {{Controlling the false discovery rate via knockoffs}},
volume = {43},
year = {2015}
}
@article{Gijbels2004,
abstract = {The objective of this article is to test whether or not there is an abrupt change in the regression function itself or in its first derivative at certain (prespecified or not) locations. The test does not rely on asymptotics but approximates the sample distribution of the test statistic using a bootstrap procedure. The proposed testing method involves a data-driven choice of the smoothing parameters. The performance of the testing procedures is evaluated via a simulation study. Some comparison with an asymptotic test by Hamrouni (1999) and Gr{\'{e}}goire and Hamrouni (2002b) and asymptotic tests by M{\"{u}}ller and Stadtm{\"{u}}ller (1999) and Dubowik and Stadtm{\"{u}}ller (2000) is provided. We also demonstrate the use of the testing procedures on some real data.},
author = {Gijbels, I. and Goderniaux, A-C.},
doi = {10.1080/10485250310001626088},
file = {::},
isbn = {1048525031},
issn = {1048-5252},
journal = {Journal of Nonparametric Statistics},
keywords = {bandwidth,bootstrap,cross-validation,derivative,discontinuity points,least-squares fitting,local,polynomial approximation},
number = {3-4},
pages = {591--611},
title = {{Bootstrap test for change-points in nonparametric regression}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10485250310001626088},
volume = {16},
year = {2004}
}
@inproceedings{gray2003very,
author = {Gray, Alexander G and Moore, Andrew W},
booktitle = {Joint Stat. Meeting},
title = {{Very fast multivariate kernel density estimation via computational geometry}},
year = {2003}
}
@article{Muller1999a,
author = {Muller, Hans-Georg and Stadtmuller, Ulrich},
file = {::},
journal = {The Annals of Statistics},
number = {1},
pages = {299--337},
title = {{Discontinuous Versus Smooth Regression}},
volume = {27},
year = {1999}
}
@article{Rice1984,
abstract = {This paper is concerned with the problem of choosing a bandwidth parameter for nonparametric regression. We analyze a tapered Fourier series estimate and discuss the relationship of this estimate to a kernel estimate. We first consider a method based on an unbiased estimate of mean square error, and show that the bandwidth thus chosen is asymptotically optimal. Other methods are examined as well and are shown to be asymptotically equivalent. A small simulation shows, however, that for small or moderate sample size, the methods perform quite differently.},
author = {Rice, John},
issn = {00905364},
journal = {The Annals of Statistics},
number = {4},
pages = {1215--1230},
publisher = {Institute of Mathematical Statistics},
title = {{Bandwidth Choice for Nonparametric Regression}},
url = {http://www.jstor.org/stable/2240998},
volume = {12},
year = {1984}
}
@misc{Buchman2011,
abstract = {We present nonparametric techniques for constructing and verifying density estimates from high-dimensional data whose irregular dependence structure cannot be modelled by parametric multivariate distributions. A low-dimensional representation of the data is critical in such situations because of the curse of dimensionality. Our proposed methodology consists of three main parts: (1) data reparameterization via dimensionality reduction, wherein the data are mapped into a space where standard techniques can be used for density estimation and simulation; (2) inverse mapping, in which simulated points are mapped back to the high-dimensional input space; and (3) verification, in which the quality of the estimate is assessed by comparing simulated samples with the observed data. These approaches are illustrated via an exploration of the spatial variability of tropical cyclones in the North Atlantic; each datum in this case is an entire hurricane trajectory. We conclude the paper with a discussion of extending the methods to model the relationship between TC variability and climatic variables. {\textcopyright} 2009 Elsevier B.V.},
author = {Buchman, Susan M. and Lee, Ann B. and Schafer, Chad M.},
booktitle = {Statistical Methodology},
doi = {10.1016/j.stamet.2009.07.002},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/High-dimensional density estimation via SCA An example in the modelling of hurricane tracks .pdf:pdf},
issn = {15723127},
keywords = {Application to physical sciences,Dimension reduction,Nonparametric density estimation},
number = {1},
pages = {18--30},
title = {{High-dimensional density estimation via SCA: An example in the modelling of hurricane tracks}},
volume = {8},
year = {2011}
}
@book{Pagan2005,
annote = {这本书是介绍 GMM 的好书, "慧航"推荐, 有空一定要看看.
GMM 对于计量/金融 十分的重要, 大部分学者都是在GMM的框架下工作的.

?: 难点在于弱工具变量.},
author = {Pagan, Adrian},
doi = {10.1016/S0167-7306(08)60305-1},
file = {::},
isbn = {9780444805799},
issn = {01677306},
keywords = {GMM},
mendeley-tags = {GMM},
publisher = {Oxford Press},
title = {{Generalized Method of Moments}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167730608603051},
year = {2005}
}
@article{Dai2014,
abstract = {Variance estimation is an important topic in nonparametric regression. In this paper, we propose a pairwise regression method for estimating the residual variance. Specifically, we regress the squared difference between observations on the squared distance between design points, and then estimate the residual variance as the intercept. Unlike most existing difference-based estimators that require a smooth regression function, our method applies to regression models with jump discontinuities. Our method also applies to the situations where the design points are unequally spaced. Finally, we conduct extensive simulation studies to evaluate the finite-sample performance of the proposed method and compare it with some existing competitors. {\textcopyright} 2013 {\textcopyright} 2013 Taylor  {\&}  Francis.},
author = {Dai, Wenlin and Tong, Tiejun},
doi = {10.1080/02664763.2013.842962},
file = {::},
issn = {02664763},
journal = {Journal of Applied Statistics},
keywords = {difference-based estimator,jump point,non-uniform design,nonparametric regression,pairwise regression,residual variance},
number = {3},
pages = {530--545},
title = {{Variance estimation in nonparametric regression with jump discontinuities}},
volume = {41},
year = {2014}
}
@article{Tong2005,
author = {Tong, Tiejun and Wang, Yuedong},
doi = {10.1093/biomet/92.4.821},
file = {::},
issn = {00063444},
journal = {Biometrika},
keywords = {Bandwidth,Difference-based estimator,Least squares,Nonparametric regression,Quadratic form,Residual variance},
number = {4},
pages = {821--830},
title = {{Estimating residual variance in nonparametric regression using least squares}},
volume = {92},
year = {2005}
}
@article{page1954continuous,
author = {Page, Ewan S},
journal = {Biometrika},
number = {1/2},
pages = {100--115},
publisher = {JSTOR},
title = {{Continuous inspection schemes}},
volume = {41},
year = {1954}
}
@article{Shiau1986Partial,
author = {Shiau, Jyh Jen and Wahba, Grace and Johnson, Donald R},
journal = {Journal of Atmospheric {\&} Oceanic Technology},
number = {4},
pages = {714--725},
title = {{Partial Spline Models for the Inclusion of Tropopause and Frontal Boundary Information in Otherwise Smooth Two- and Three-Dimensional Objective Analysis}},
volume = {3},
year = {1986}
}
@article{Paul1989,
annote = {里面有一个添加节点的三角剖分算法
似乎挺有意思的.},
author = {Paul, Bourke},
file = {::},
title = {{Efficient Triangulation Algorithm Suitable for Terrain Modelling}},
year = {1989}
}
@misc{Hu2007,
abstract = {Learning the underlying model from distributed data is often useful for many distributed systems. In this paper, we study the problem of learning a non-parametric model from distributed observations. We propose a gossip-based distributed kernel density estimation algorithm and analyze the convergence and consistency of the estimation process. Furthermore, we extend our algorithm to distributed systems under communication and storage constraints by introducing a fast and efficient data reduction algorithm. Experiments show that our algorithm can estimate underlying density distribution accurately and robustly with only small communication and storage overhead. {\textcopyright} 2007 IEEE.},
author = {Hu, Yusuo and Chen, Hua and Lou, Jian Guang and Li, Jiang},
booktitle = {Proceedings - International Conference on Distributed Computing Systems},
doi = {10.1109/ICDCS.2007.100},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Distributed Density Estimation Using Non-parametric Statistics.pdf:pdf},
isbn = {0769528376},
keywords = {Data reduction,Distributed estimation,Gossip,Kernel density estimation,Non-parametric statistics},
title = {{Distributed density estimation using non-parametric statistics}},
year = {2007}
}
@article{Luft1948,
abstract = {For the change-point model with at most one change X(i/n) = f(i/n) + $\epsilon$(i/n), wherendently and identically distributed, the distribution of the estimator of the change point proposed in this paper can be approaximated by the first type of the extremal distribution with the help of the theory of Gaussian process. The problem of testing and interval estimation about the change-point to the magnitude of jump ($\alpha$2-$\alpha$1) and the magnitude of slope change ($\beta$2 - $\beta$1) are considered.},
author = {Tan, Zhiping},
file = {::},
journal = {Chinese Journal of Applied Probability and Statisties},
number = {3},
title = {{Statistical Inference in a Model with at Most One Change-point}},
year = {1996}
}
@article{Bhojanapalli2016,
abstract = {We show that there are no spurious local minima in the non-convex factorized parametrization of low-rank matrix recovery from incoherent linear measurements. With noisy measurements we show all local minima are very close to a global optimum. Together with a curvature bound at saddle points, this yields a polynomial time global convergence guarantee for stochastic gradient descent {\{}$\backslash$em from random initialization{\}}.},
archivePrefix = {arXiv},
arxivId = {1605.07221},
author = {Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nathan},
eprint = {1605.07221},
file = {::},
issn = {10495258},
number = {2},
pages = {1--9},
title = {{Global Optimality of Local Search for Low Rank Matrix Recovery}},
url = {http://arxiv.org/abs/1605.07221},
year = {2016}
}
@article{Chernozhukov2016,
abstract = {Most modern supervised statistical/machine learning (ML) methods are explicitly designed to solve prediction problems very well. Achieving this goal does not imply that these methods automatically deliver good estimators of causal parameters. Examples of such parameters include individual regression coefficients, average treatment effects, average lifts, and demand or supply elasticities. In fact, estimates of such causal parameters obtained via naively plugging ML estimators into estimating equations for such parameters can behave very poorly due to the regularization bias. Fortunately, this regularization bias can be removed by solving auxiliary prediction problems via ML tools. Specifically, we can form an orthogonal score for the target low-dimensional parameter by combining auxiliary and main ML predictions. The score is then used to build a de-biased estimator of the target parameter which typically will converge at the fastest possible 1/root(n) rate and be approximately unbiased and normal, and from which valid confidence intervals for these parameters of interest may be constructed. The resulting method thus could be called a "double ML" method because it relies on estimating primary and auxiliary predictive models. In order to avoid overfitting, our construction also makes use of the K-fold sample splitting, which we call cross-fitting. This allows us to use a very broad set of ML predictive methods in solving the auxiliary and main prediction problems, such as random forest, lasso, ridge, deep neural nets, boosted trees, as well as various hybrids and aggregators of these methods.},
annote = {Question
1. Unconfoundedness aaumption
2. What's the difference between sample splitting {\&} cross-fitting
3. The Dim of X in example is not large enough.
4. Difference between Double ML? Does it use the same method?
5. P25, what's 'a' ?? Answer: P24, a is a notation for slope part of psi.},
archivePrefix = {arXiv},
arxivId = {1608.00060},
author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
doi = {10.1920/wp.cem.2016.4916},
eprint = {1608.00060},
file = {::},
issn = {0002-8282},
journal = {Cemmap Working Papers},
title = {{Double/Debiased Machine Learning for Treatment and Causal Parameters}},
url = {http://arxiv.org/abs/1608.00060},
year = {2016}
}
@article{hjort1996locally,
author = {Hjort, Nils Lid and Jones, M Chris},
journal = {The Annals of Statistics},
pages = {1619--1647},
publisher = {JSTOR},
title = {{Locally parametric nonparametric density estimation}},
year = {1996}
}
@article{Killick2013,
abstract = {This article proposes a test to detect changes in general autoco-variance structure in nonstationary time series. Our approach is founded on the locally stationary wavelet (LSW) process model for time series which has previously been used for classification and segmentation of time se-ries. Using this framework we form a likelihood-based hypothesis test and demonstrate its performance against existing methods on various simulated examples as well as applying it to a problem arising from ocean engineering.},
author = {Killick, R. and Eckley, I. A. and Jonathan, P.},
doi = {10.1214/13-EJS799},
file = {::},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Local stationarity,Segmentation,Significant wave height,Wavelets},
number = {1},
pages = {1167--1183},
title = {{A wavelet-based approach for detecting changes in second order structure within nonstationary time series}},
volume = {7},
year = {2013}
}
@article{Seidel1995,
abstract = {Since at least half of the d edges incident to a vertex v of a simple d-polytope P either all point "up" or all point "down," v must be the unique "bottom" or "top" vertex of a face of P of dimension at least d 2. Thus the number of P's vertices is at most twice the number of such high-dimensional faces, which is at most ∑d/2 ≤ i ≤ d( n d-v) = 0(n⌊d/2⌋), if P has n facets. This, in a nutshell, provides a proof of the asymptotic version of the famous upper bound theorem: A convex d-polytope with n facets (or, dually, with n vertices) has O(n[ d 2]) faces, when d is constant. {\textcopyright} 1995.},
annote = {i-face 是什么东西?
1-face 是点
2-face是线
3-face是面
4-face是四面体},
author = {Seidel, Raimund},
doi = {10.1016/0925-7721(95)00013-Y},
file = {::},
issn = {09257721},
journal = {Computational Geometry: Theory and Applications},
number = {2},
pages = {115--116},
title = {{The upper bound theorem for polytopes: an easy proof of its asymptotic version}},
volume = {5},
year = {1995}
}
@unpublished{zhao2019Multi,
author = {Zhao, Saijun and Zheng, Zemin},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/高维统计分析/Saijun Zhao/document7.pdf:pdf},
institution = {USTC},
title = {{Multivariate Knockoff Filter for Dimension Reduction and Controlled Variable Selection in Multivariate Regression}},
year = {2019}
}
@misc{Wang1995,
abstract = {A method is proposed to detect jumps and sharp cusps in a function which is observed with noise, by checking if the wavelet transformation of the data has significantly large absolute values across fine scale levels. Asymptotic theory is established and practical implementation is discussed. The method is tested on simulated examples, and applied to stock market return data.},
author = {Wang, Yazhen},
booktitle = {Biometrika},
doi = {10.1093/biomet/82.2.385},
file = {::},
issn = {00063444},
keywords = {Convergence rate,Estimation,Hypothesis,Jump,Nonparametric regression,Sharp cusp,Wavelet transformation,White noise,White noise model},
number = {2},
pages = {385--397},
title = {{Jump and sharp cusp detection by wavelets}},
volume = {82},
year = {1995}
}
@article{Yu2003,
abstract = {Feature selection, as a preprocessing step to machine learning, is effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection methods with respect to efficiency and effectiveness. In this work, we introduce a novel concept, predominant correlation, and propose a fast filter method which can identify relevant features as well as redundancy among relevant features without pairwise correlation analysis. The efficiency and effectiveness of our method is demonstrated through extensive comparisons with other methods using real-world data of high dimensionality.},
author = {Yu, Lei and Liu, Huan},
doi = {citeulike-article-id:3398512},
file = {::},
isbn = {1577351894},
issn = {01469592},
journal = {International Conference on Machine Learning (ICML)},
pages = {1--8},
title = {{Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution}},
url = {http://www.aaai.org/Papers/ICML/2003/ICML03-111.pdf},
year = {2003}
}
@article{Engle1986Semiparametric,
author = {Engle, Robert F and Granger, C W J and Rice, John and Weiss, Andrew},
journal = {Publications of the American Statistical Association},
number = {394},
pages = {310--320},
title = {{Semiparametric estimates of the relation between weather and electricity sales}},
volume = {81},
year = {1986}
}
@article{Fraley2007,
abstract = {Due to recent advances in methods and software for model-based clustering, and to the interpretability of the results, clustering procedures based on probability models are increasingly preferred over heuristic methods. The clustering process estimates a model for the data that allows for overlapping clusters, producing a probabilistic clustering that quantifies the uncertainty of observations belonging to components of the mixture. The resulting clustering model can also be used for some other important problems in multivariate analysis, including density estimation and discriminant analysis. Examples of the use of model-based clustering and classification techniques in chemometric studies include multivariate image analysis, magnetic resonance imaging, microarray image segmentation, statistical process control, and food authenticity. We review model-based clustering and related methods for density estimation and discriminant analysis, and show how the R package mclust can be applied in each instance.},
author = {Fraley, Chris and Raftery and Adrian, E},
doi = {10.18637/jss.v018.i06},
file = {::},
isbn = {1548-7660},
issn = {15487660},
journal = {Journal Of Statistical Software},
keywords = {classification,density estimation,discriminant analysis,model based clustering,r},
number = {6},
pages = {1--13},
title = {{Model-based Methods of Classification : Using the mclust Software in Chemometrics}},
url = {http://www.jstatsoft.org/v18/i06/paper/},
volume = {18},
year = {2007}
}
@article{Tan,
author = {Tan, Changchun and Chen, Si and Miao, Baiqi},
doi = {10.3969/j.issn.0253-2778.2011.09.004},
file = {::},
journal = {Journal of University of Science and Technology of China},
title = {{Strong convergence rate of jump-slop change-point estimation}},
year = {2011}
}
@misc{Rouigueb2014,
abstract = {This paper proposes a semi-non parametric density estimation framework for high-dimensional data. Dimensionality reduction is achieved by reorganizing the domain variables set into a junction tree of cliques each containing a small number of variables where factorization of the joint density into a tree is carried out by learning the Bayesian Network (BN) structure graph and by searching the maximum spanning tree over the moralized-triangulated graph of the obtained BN. To estimate the density of the junction tree elements, we propose a novel technique using local Independent Component Analysis (ICA) method based on fuzzy clustering. The main contribution relates to the development of a generic framework through a combination of three complimentary modules: (1) BN structure learning, (2) fuzzy clustering, and (3) linear ICA method. This allows us to exploit the separation power of recently developed ICA tools. Hence, depending on the data characteristics, the user can choose among a wide range of ICA and BN tools the most suitable one. We experimentally evaluated our approach in a supervised classification problem and the obtained results indicate an improvement in accuracy. {\textcopyright} 2014 IOS Press and the authors. All rights reserved.},
author = {Rouigueb, Abdenebi and Chitroub, Salim and Bouridane, Ahmed},
booktitle = {Intelligent Data Analysis},
doi = {10.3233/IDA-140635},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Density estimation of high dimensional data using ICA and Bayesian networks.pdf:pdf},
issn = {1088467X},
keywords = {Bayesian network,Local ICA,density estimation,fuzzy clustering,high dimension},
number = {2},
pages = {157--179},
title = {{Density estimation of high dimensional data using ICA and Bayesian networks}},
volume = {18},
year = {2014}
}
@article{Bliznyuk2012,
abstract = {Estimation of covariance function parameters of the error process in the presence of an unknown smooth trend is an important problem because solving it allows one to estimate the trend nonparametrically using a smoother corrected for dependence in the errors. Our work is motivated by spatial statistics but is applicable to other contexts where the dimension of the index set can exceed one. We obtain an estimator of the covariance function parameters by regressing squared differences of the response on their expectations, which equal the variogram plus an offset term induced by the trend. Existing estimators that ignore the trend produce bias in the estimates of the variogram parameters, which our procedure corrects for. Our estimator can be justified asymptotically under the increasing domain framework. Simulation studies suggest that our estimator compares favorably with those in the current literature while making less restrictive assumptions. We use our method to estimate the variogram parameters of the short-range spatial process in a U. S. precipitation data set.},
author = {Bliznyuk, Nikolay and Carroll, Raymond J. and Genton, Marc G. and Wang, Yuedong},
doi = {10.4310/SII.2012.v5.n2.a2},
file = {::},
issn = {1938-7989},
journal = {Statistics and Its Interface},
keywords = {Bias,Covariance function,ERRORS,FIELDS,LEAST-SQUARES,NONPARAMETRIC REGRESSION,Nonlinear regression,Nonparametric regression,Spatio-temporal dependence,Time series},
number = {2},
pages = {159--168},
pmid = {22737256},
title = {{Variogram estimation in the presence of trend}},
url = {http://apps.webofknowledge.com/full{\_}record.do?product=WOS{\&}search{\_}mode=CitingArticles{\&}qid=70{\&}SID=Z12XnLkkPbSOPpKl99k{\&}page=5{\&}doc=47},
volume = {5},
year = {2012}
}
@article{Neumann1941Distribution,
author = {Neumann, John Von},
journal = {Annals of Mathematical Statistics},
number = {4},
pages = {367--395},
title = {{Distribution of the Ratio of the Mean Square Successive Difference to the Variance}},
volume = {12},
year = {1941}
}
@article{Bhojanapalli2015,
abstract = {We study the minimization of a convex function {\$}f(X){\$} over the set of {\$}n\backslashtimes n{\$} positive semi-definite matrices, but when the problem is recast as {\$}\backslashmin{\_}U g(U) := f(UU{\^{}}\backslashtop){\$}, with {\$}U \backslashin \backslashmathbb{\{}R{\}}{\^{}}{\{}n \backslashtimes r{\}}{\$} and {\$}r \backslashleq n{\$}. We study the performance of gradient descent on {\$}g{\$}---which we refer to as Factored Gradient Descent (FGD)---under standard assumptions on the original function {\$}f{\$}. We provide a rule for selecting the step size and, with this choice, show that the local convergence rate of FGD mirrors that of standard gradient descent on the original {\$}f{\$}: i.e., after {\$}k{\$} steps, the error is {\$}O(1/k){\$} for smooth {\$}f{\$}, and exponentially small in {\$}k{\$} when {\$}f{\$} is (restricted) strongly convex. In addition, we provide a procedure to initialize FGD for (restricted) strongly convex objectives and when one only has access to {\$}f{\$} via a first-order oracle; for several problem instances, such proper initialization leads to global convergence guarantees. FGD and similar procedures are widely used in practice for problems that can be posed as matrix factorization. To the best of our knowledge, this is the first paper to provide precise convergence rate guarantees for general convex functions under standard convex assumptions.},
annote = {这篇文章的创新点在于, 针对 semi-definite 矩阵的分解梯度下降法(FGD), 在梯度下降法的基础之上, 给出了迭代步长选择, 以及初值选择的新方法.
性质:
可以考虑非凸函数的优化(?)},
archivePrefix = {arXiv},
arxivId = {1509.03917},
author = {Bhojanapalli, Srinadh and Kyrillidis, Anastasios and Sanghavi, Sujay},
eprint = {1509.03917},
file = {::},
keywords = {non-convex analysis and optimization,rank minimization,semi-definite matrix},
number = {1},
pages = {1--53},
title = {{Dropping Convexity for Faster Semi-definite Optimization}},
url = {http://arxiv.org/abs/1509.03917},
volume = {49},
year = {2015}
}
@article{Huang2017,
abstract = {We develop a constructive approach to estimating sparse, high-dimensional linear regression models. The approach is a computational algorithm motivated from the KKT conditions for the {\$}\backslashell{\_}0{\$}-penalized least squares solutions. It generates a sequence of solutions iteratively, based on support detection using primal and dual information and root finding. We refer to the algorithm as SDAR for brevity. Under a sparse Rieze condition on the design matrix and certain other conditions, we show that with high probability, the {\$}\backslashell{\_}2{\$} estimation error of the solution sequence decays exponentially to the minimax error bound in {\$}O(\backslashsqrt{\{}J{\}}\backslashlog(R)){\$} steps; and under a mutual coherence condition and certain other conditions, the {\$}\backslashell{\_}{\{}\backslashinfty{\}}{\$} estimation error decays to the optimal error bound in {\$}O(\backslashlog(R)){\$} steps, where {\$}J{\$} is the number of important predictors, {\$}R{\$} is the relative magnitude of the nonzero target coefficients. Computational complexity analysis shows that the cost of SDAR is {\$}O(np){\$} per iteration. Moreover the oracle least squares estimator can be exactly recovered with high probability at the same cost if we know the sparsity level. We also consider an adaptive version of SDAR to make it more practical in applications. Numerical comparisons with Lasso, MCP and greedy methods demonstrate that SDAR is competitive with or outperforms them in accuracy and efficiency.},
annote = {线性模型变量选择 L{\_}0 惩罚
基于 KKT 条件, 构造迭代scheme, 加速 L{\_}0 惩罚函数的计算速度. 

这本质上是一个计算的算法. 速度较快.},
archivePrefix = {arXiv},
arxivId = {1701.05128},
author = {Huang, Jian and Jiao, Yuling and Liu, Yanyan and Lu, Xiliang},
eprint = {1701.05128},
file = {::},
keywords = {0 penalization,62G05,62N02,Adaptive thresholding,global geometrical convergence,root finding.,support detection},
month = {jan},
title = {{A Constructive Approach to High-dimensional Regression}},
url = {http://arxiv.org/abs/1701.05128},
year = {2017}
}
@misc{Gu2013,
abstract = {Penalized likelihood density estimation provides an effective approach to the nonparametric fitting of graphical models, with conditional independence structures characterized via selective term elimination in functional ANOVA decompositions of the log density. A bottleneck in the approach has been the cost of numerical integration, which has limited its application to low-dimensional problems. In Jeon and Lin (2006), a reformulation was proposed to replace multi-dimensional integrals by sums of products of univariate integrals, greatly reducing the numerical burden in high-dimensional problems. In this article, we derive a cross-validation score for use with the reformulation that delivers effective smoothing parameter selection at a manageable computational cost, introduce a geometric inference tool for the "testing" of model terms, and calculate the asymptotic convergence rates of the estimates. An assortment of practical issues are investigated through empirical studies, and open-source software is illustrated with data examples.},
author = {Gu, Chong and Jeon, Yongho and Lin, Yi},
booktitle = {Statistica Sinica},
doi = {10.5705/ss.2011.319},
file = {:C$\backslash$:/Users/gnius/OneDrive/Statistics/工具{\&}课程{\&}书籍/Nonparamter/Essays/High-Dim KDE/Ref/Nonparametric Density Estimation in High-Dimensions.pdf:pdf},
issn = {10170405},
keywords = {Cross-validation,Graphical models,Penalized likelihood,Projection,Smoothing parameter},
number = {3},
pages = {1131--1153},
title = {{Nonparametric density estimation in high-dimensions}},
volume = {23},
year = {2013}
}
@article{loader1996local,
author = {Loader, Clive R and Others},
journal = {The Annals of Statistics},
number = {4},
pages = {1602--1618},
publisher = {Institute of Mathematical Statistics},
title = {{Local likelihood density estimation}},
volume = {24},
year = {1996}
}
@article{Tong2013,
abstract = {We study the least squares estimator in the residual variance estimation context. We show that the mean squared differences of paired observations are asymptotically normally distributed. We further establish that, by regressing the mean squared differences of these paired observations on the squared distances between paired covariates via a simple least squares procedure, the resulting variance estimator is not only asymptotically normal and root-n consistent, but also reaches the optimal bound in terms of estimation variance. We also demonstrate the advantage of the least squares estimator in comparison with existing methods in terms of the second order asymptotic properties.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.3046v1},
author = {Tong, Tiejun and Ma, Yanyuan and Wang, Yuedong},
doi = {10.3150/12-BEJ432},
eprint = {arXiv:1312.3046v1},
file = {::},
issn = {1350-7265},
journal = {Bernoulli},
keywords = {asymptotic normality,difference-based estimator,generalized least squares,nonparametric,optimal bound,regression,residual variance},
number = {5A},
pages = {1839--1854},
title = {{Optimal variance estimation without estimating the mean function}},
url = {http://projecteuclid.org/euclid.bj/1383661205},
volume = {19},
year = {2013}
}
@article{Dai2018,
abstract = {{\textcopyright} 2017 Informa UK Limited, trading as Taylor  {\&}  Francis Group. In nonparametric regression, it is often needed to detect whether there are jump discontinuities in the mean function. In this paper, we revisit the difference-based method in [13] and propose to further improve it. To achieve the goal, we first reveal that their method is less efficient due to the inappropriate choice of the response variable in their linear regression model. We then propose a new regression model for estimating the residual variance and the total amount of discontinuities simultaneously. In both theory and simulation, we show that the proposed variance estimator has a smaller mean-squared error compared to the existing estimator, whereas the estimation efficiency for the total amount of discontinuities remains unchanged. Finally, we construct a new test procedure for detection of discontinuities using the proposed method; and via simulation studies, we demonstrate that our new test procedure outperforms the existing one in most settings.},
author = {Dai, Wenlin and Zhou, Yuejin and Tong, Tiejun},
doi = {10.1080/02664763.2017.1280004},
file = {::},
issn = {13600532},
journal = {Journal of Applied Statistics},
keywords = {Asymptotic normality,difference-based estimator,jump point,model selection,nonparametric regression,residual variance},
number = {3},
pages = {450--473},
title = {{Testing discontinuities in nonparametric regression}},
volume = {45},
year = {2018}
}
@article{ChenXiru1991,
author = {陈希孺},
journal = {数理统计与管理},
number = {2},
pages = {52--59},
title = {变点统计分析简介},
volume = {10},
year = {1991}
}
@article{Bilias2000,
abstract = {Cost considerations and the need to report the results promptly make it desirable to examine data as it accumulates and to terminate an experimental study as soon as definite statistical conclusions can be drawn. These ideas are illustrated in a retrospective analysis of the Pennsylvania 'Reemployment Bonus' experiment. Recently developed large-sample theory for a variety of sequentially computed score processes of time-to-event (duration) data with staggered entry enables one to construct stopping boundaries of a prescribed level of significance. The gains from the use of sequential methodology appear to be worth while. Copyright {\textcopyright} 2000 John Wiley {\&} Sons, Ltd.},
author = {Bilias, Yannis},
doi = {10.1002/jae.579},
file = {::},
issn = {08837252},
journal = {Journal of Applied Econometrics},
number = {6},
pages = {575--594},
title = {{Sequential testing of duration data: The case of the Pennsylvania 'Reemployment Bonus' experiment}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-0034552809{\&}partnerID=tZOtx3y1},
volume = {15},
year = {2000}
}
@article{Hall2003,
abstract = {We show that difference-based methods can be used to construct simple and explicit estimators of error covariance and autoregressive parameters in nonparametric regression with time series errors. When the error process is Gaussian our estimators are efficient, but they are available well beyond the Gaussian case. As an illustration of their usefulness we show that difference-based estimators can be used to produce a simplified version of time series cross-validation. This new approach produces a bandwidth selector that is equivalent, to both first and second orders, to that given by the full time series cross-validation algorithm. Other applications of difference-based methods are to variance estimation and construction of confidence bands in nonparametric regression.},
author = {Hall, Peter and {Van Keilegom}, Ingrid},
doi = {10.1111/1467-9868.00395},
file = {::},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Autoregression,Bandwidth,Covariance,Cross-validation,Kernel methods,Linear time series,Local linear regression,Time series cross-validation},
number = {2},
pages = {443--456},
title = {{Using difference-based methods for inference in nonparametric regression with time series errors}},
volume = {65},
year = {2003}
}
@article{scott2006learning,
author = {Scott, Clayton D and Nowak, Robert D},
journal = {Journal of Machine Learning Research},
number = {Apr},
pages = {665--704},
title = {{Learning minimum volume sets}},
volume = {7},
year = {2006}
}
@article{Porter2015,
abstract = {The regression discontinuity design has become a common framework among applied economists for measuring treatment effects. A key restriction of the existing literature is the assumption that the discontinuity point is known, which does not always hold in practice. This paper extends the applicability of the regression discontinuity design by allowing for an unknown discontinuity point. First, we construct a unified test statistic to check whether there are selection or treatment effects. Our tests are shown to be consistent, and local powers are derived. Also, a bootstrap method is proposed to obtain critical values. Second, we estimate the treatment effect by first estimating the nuisance discontinuity point. It is shown that estimating the discontinuity point does not affect the efficiency of the treatment effect estimator. Simulation studies illustrate the usefulness of our procedures in finite samples.},
author = {Porter, Jack and Yu, Ping},
doi = {10.1016/j.jeconom.2015.06.002},
file = {::},
issn = {18726895},
journal = {Journal of Econometrics},
keywords = {Cross validation,Degenerate U-statistic,Difference kernel estimator,Nonparametric structural change,Regression discontinuity design,Specification testing,Unknown discontinuity point,Wild bootstrap},
number = {1},
pages = {132--147},
publisher = {Elsevier B.V.},
title = {{Regression discontinuity designs with unknown discontinuity points: Testing and estimation}},
url = {http://dx.doi.org/10.1016/j.jeconom.2015.06.002},
volume = {189},
year = {2015}
}
