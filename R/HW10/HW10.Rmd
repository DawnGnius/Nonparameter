---
title: "Homework 10"
author: "Liu Huihang"
date: "10/30/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("plotly")
library("parallel")
```

## Exercise 1 (HW 10.1)

For the data set **faithful** in R, use the product sum kernel and spherical symmetric kernel to estimate the joint density function of *erosions* and *waiting*, respectively, considering different bandwidth selection methods.

## Answer 1
In this case, the data lies in $\mathcal{R^2}$ thus we take $d=2$ in the following.

- Multiplicative kernel
\begin{equation}
  \mathcal{K}(\mathbf{u})
  = \left( \frac{3}{4} \right)^d \prod_{i=1}^d (1- \mu_i^2) I(|\mu_i| \le 1)
  = \frac{9}{16} \prod_{i=1}^2 (1- \mu_i^2) I(|\mu_i| \le 1)
\end{equation}

- Spherical symmetric kernel
\begin{equation}
  \mathcal{K}(\mathbf{u}) 
  = \frac{(1-\mathbf{u}^T\mathbf{u}) I(\mathbf{u}^T\mathbf{u}\le 1)}{\int_{\mathcal{R}^d} (1-\mathbf{u}^T\mathbf{u}) I(\mathbf{u}^T\mathbf{u}\le 1) d\mathbf{u}} 
  = \frac{(1-\mathbf{u}^T\mathbf{u}) I(\mathbf{u}^T\mathbf{u}\le 1)}{\pi/2} 
\end{equation}

The multivariate kernel density estimator
\begin{equation}
\hat{f}_{\mathbf{H}} (\mathbf{x}) = \frac{1}{n} \sum_{i=1}^n \frac{1}{det(\mathbf{H})} \mathcal{K} \left(\mathbf{H}^{-1} (\mathbf{x} - \mathbf{X_i}) \right)
\end{equation}

In addition, the bandwidth is choosen by
\begin{equation}
  \begin{split}
    \hat{h}_{plug-in} & = \left(\frac{4}{d+2}\right)^{1/(d+4)} \sigma_j n^{1/(d+4)}\\
    \hat{h}_{scott} & = \sigma_j n^{1/(d+4)}\\
    \hat{h}_{g.scott} & = \hat{\Sigma}^{1/2} n^{1/(d+4)}\\
  \end{split}
\end{equation}
```{r}
data("faithful")
dat <- data.matrix(faithful)
n <- dim(dat)[1]
Dim <- 2

multip.kernel <- function(u){
  # u is a vector in 2-dim
  (1-u[1])*(1-u[2])*9/16
}

sphi.kernel <- function(u){
  # u is a vector in 2-dim
  (1-t(u)%*%u)*2/pi
}

kde.multiple <- function(u, H){
  fhx <- 0
  for (ii in 1:n) {
    xx <- solve(H) %*% (u - dat[ii, ])
    if (abs(xx[1]) <=1 & abs(xx[2]) <=1)
      fhx <- fhx + 1
  }
  fhx <- fhx / n / det(H)
}

kde.sphi <- function(u, H){
  fhx <- 0
  for (ii in 1:n) {
    xx <- solve(H) %*% (u - dat[ii, ])
    if (norm(xx, "2") <= 1)
      fhx <- fhx + 1
  }
  fhx <- fhx / n / det(H)
}

x <- seq(1, 5.5, length.out=30)
y <- seq(40, 100, length.out=30)
f.hat.multiple <- matrix(rep(0, length(x)*length(y)), nrow=length(x))
f.hat.sphi <- matrix(rep(0, length(x)*length(y)), nrow=length(x))

# bandwith selection
Sig <- cov(dat)
H.plug_in <- diag(diag(sqrt(Sig))) * n^(-1/6) * (4 / 6)^(1/6)
H.scott <- diag(diag(sqrt(Sig))) * n^(-1/6)

svd_dat <- svd(Sig)
H.g_scott <- svd_dat$u %*% diag(sqrt(svd_dat$d)) %*% t(svd_dat$v) * n^(-1/6)

estimate <- function(H){
  # single core code
  for(ii in 1:length(x)){
    for (jj in 1:length(y)) {
      u <- c(x[ii], y[jj])
      f.hat.multiple[ii, jj] <- kde.multiple(u, H)
      f.hat.sphi[ii, jj] <- kde.sphi(u, H)
    }
  }
  
  plot_ly() %>% add_surface(z = ~f.hat.multiple)
  
  plot_ly() %>% add_surface(z = ~f.hat.sphi)
}

estimate(H.plug_in)
estimate(H.scott)
estimate(H.g_scott)

```


## Exercise 2 (HW 10.3)
For the **wines** data, remove the categorical variable vintage and normalize the variables, then perform PCA. 
Using the clustering method based on density estimation (refer to Appendix cluster.R),

(1) How many classes can be found using 3 principal components? 
What happens if the *min.clust.size* variable is set to a minimum class containing at least $5\%$ of the data? 
Is the result sensitive to the value of *min.clust.size*?

(2) Using the same min.clust.size value, using $6$ principal components, can you find several classes? 
Use a two-two scatter plot to identify the principal components that contribute to the clustering, as well as the principal components that are not helpful.

(3) Using two principal components for clustering, can you find several classes? 
Compare the results of $2$ principal components and $3$ principal components.

(4) Compare clustering results with real categories.

(5) Compare the results of $k=3, 4$ under the kmeans clustering algorithm.

## Answer 2

(1)

```{r, include=FALSE}
library("numDeriv")
library("viridis")
# Planets
th <- 2 * pi / 3
r <- 2
xi_1 <- r * c(cos(th + 0.5), sin(th + 0.5))
xi_2 <- r * c(cos(2 * th + 0.5), sin(2 * th + 0.5))
xi_3 <- r * c(cos(3 * th + 0.5), sin(3 * th + 0.5))

# Gravity force
gravity <- function(x) {

  (mvtnorm::dmvnorm(x = x, mean = xi_1, sigma = diag(rep(0.5, 2))) +
     mvtnorm::dmvnorm(x = x, mean = xi_2, sigma = diag(rep(0.5, 2))) +
     mvtnorm::dmvnorm(x = x, mean = xi_3, sigma = diag(rep(0.5, 2)))) / 3

}

# Compute numerically the gradient of an arbitrary function
attraction <- function(x) numDeriv::grad(func = gravity, x = x)

# Evaluate the vector field
x <- seq(-4, 4, l = 20)
xy <- expand.grid(x = x, y = x)
dir <- apply(xy, 1, attraction)

# Scale arrows to unit length for better visualization
len <- sqrt(colSums(dir^2))
dir <- 0.25 * scale(dir, center = FALSE, scale = len)

# Colors of the arrows according to their original magnitude
cuts <- cut(x = len, breaks = quantile(len, probs = seq(0, 1, length.out = 21)))
cols <- viridis::viridis(20)[cuts]

# Vector field plot
plot(0, 0, type = "n", xlim = c(-4, 4), ylim = c(-4, 4), xlab = "x", ylab = "y")
arrows(x0 = xy$x, y0 = xy$y, x1 = xy$x + dir[1, ], y1 = xy$y + dir[2, ],
       angle = 10, length = 0.1, col = cols, lwd = 2)
points(rbind(xi_1, xi_2, xi_3), pch = 19, cex = 1.5)



################################################################
# A simulated example for which the population clusters are known
# Extracted from ?ks::dmvnorm.mixt
mus <- rbind(c(-1, 0), c(1, 2 / sqrt(3)), c(1, -2 / sqrt(3)))
Sigmas <- 1/25 * rbind(ks::invvech(c(9, 63/10, 49/4)),
                       ks::invvech(c(9, 0, 49/4)),
                       ks::invvech(c(9, 0, 49/4)))
props <- c(3, 3, 1) / 7

# Sample the mixture
set.seed(123456)
x <- ks::rmvnorm.mixt(n = 1000, mus = mus, Sigmas = Sigmas, props = props)

# Kernel mean shift clustering. If H is not specified, then
# H = ks::Hpi(x, deriv.order = 1) is employed. Its computation may take some
# time, so it is advisable to compute it separately for later reusage
H <- ks::Hpi(x = x, deriv.order = 1)
kms <- ks::kms(x = x, H = H)

# Plot clusters
plot(kms, col = viridis::viridis(kms$nclust), pch = 19, xlab = "x", ylab = "y")


# Summary
summary(kms)
## Number of clusters = 3 
## Cluster label table = 521 416 63 
## Cluster modes =
##           V1          V2
## 1  1.0486466  0.96316436
## 2 -1.0049258  0.08419048
## 3  0.9888924 -1.43852908

# Objects in the kms object
kms$nclust # Number of clusters found
## [1] 3
kms$nclust.table # Sizes of clusters
## 
##   1   2   3 
## 521 416  63
kms$mode # Estimated modes
##            [,1]        [,2]
## [1,]  1.0486466  0.96316436
## [2,] -1.0049258  0.08419048
## [3,]  0.9888924 -1.43852908

# With keep.path = TRUE the ascending paths are returned
kms <- ks::kms(x = x, H = H, keep.path = TRUE)
cols <- viridis::viridis(kms$nclust, alpha = 0.5)[kms$label]
plot(x, col = cols, pch = 19, xlab = "x", ylab = "y")
for (i in 1:nrow(x)) lines(kms$path[[i]], col = cols[i])
points(kms$mode, pch = 8, cex = 2, lwd = 2)
```


```{r}
dat <- read.table('wines.txt')
truth <- dat[-1, 14]
dat <- data.matrix(dat[-1, ][, -14])
dat <- scale(dat, center = TRUE, scale = TRUE)

fit.pca <- prcomp(dat)
pcs <- fit.pca$x

H <- ks::Hpi(x = pcs[, 1:3], deriv.order = 1)
kms_wines <- ks::kms(x = pcs[, 1:3], H = H)
cat('number of clusters: ', kms_wines$nclust, '\n')

kms_wines <- ks::kms(x = pcs[, 1:3], H = H, min.clust.size =  nrow(dat)*0.05)
cat('number of clusters: ', kms_wines$nclust, '\n')

kms_wines <- ks::kms(x = pcs[, 1:3], H = H, min.clust.size =  nrow(dat)*0.1)
cat('number of clusters: ', kms_wines$nclust, '\n')

kms_wines <- ks::kms(x = pcs[, 1:3], H = H, min.clust.size =  nrow(dat)*0.02)
cat('number of clusters: ', kms_wines$nclust, '\n')
```

If we use 3 PCs to cluster, we can get $6$ classes by default parameter. 

If the *min.clust.size* variable is set to a minimum class containing at least $5\% (9)$ of the data the number of clusters is $3$. 
If the *min.clust.size* variable is set to a minimum class containing at least $10\% (18)$ of the data the number of clusters in $3$. 
If the *min.clust.size* variable is set to a minimum class containing at least $2\% (4)$ of the data the number of clusters in $4$. 

So the result sensitive to the value of *min.clust.size*. We need to choose a appropriate value to cluster, otherwise we may get a wrong answer. It's conserative to get a small value.

(2)
We use at least $5\%$ data in each cluster and $6$ PCs.
```{r}
H <- ks::Hpi(x = pcs[, 1:6]) # Error: cannot allocate vector of size 4.5 Gb
kms_wines <- ks::kms(x = pcs[, 1:6], H = H, min.clust.size = nrow(dat)*0.05)
cat('number of clusters: ', kms_wines$nclust, '\n')
```

We get $12$ clusters.

```{r, echo=FALSE}
library(ggplot2)
library(GGally)
ggpairs(data.frame(pcs), columns=1:6, aes(color=truth)) +   ggtitle("Anderson's Iris Data -- 3 species")
```

The plot shows that PC5 and PC6 has the same pattern with any other PCs, thus these PCs may does not help to cluster. 

(3)
```{r}
H <- ks::Hpi(x = pcs[, 1:2]) # Error: cannot allocate vector of size 4.5 Gb
kms_wines <- ks::kms(x = pcs[, 1:2], H = H)
cat('number of clusters: ', kms_wines$nclust, '\n')
```
There are $5$ clusters when using $3$ PCs.

(4)
```{r}
H <- ks::Hpi(x = pcs[, 1:3], deriv.order = 1)
kms_wines <- ks::kms(x = pcs[, 1:3], H = H, min.clust.size=9)
library(mclust)
err <- classError(kms_wines$label, truth)$errorRate
cat('Clustering error rate: ', err, '\n')

kms_wines <- ks::kms(x = pcs[, 1:3], H = H)
library(mclust)
err <- classError(kms_wines$label, truth)$errorRate
cat('Clustering error rate: ', err, '\n')

H <- ks::Hpi(x = pcs[, c(1,2,3,4,6)])
kms_wines <- ks::kms(x = pcs[, c(1,2,3,4,6)], H = H, min.clust.size=9)
library(mclust)
err <- classError(kms_wines$label, truth)$errorRate
cat('Clustering error rate: ', err, '\n')
```

We use $3$ PCs and set each clusters contains $5\%$ of data, the result shows the error rate of clustering is $7.3\%$. And the error rate increases if we don't restrict the *min.clust.size*. 

In addition, if we use $5$ PCs, the error rate becomes unacceptable. 

(5)
```{r}
set.seed(1111)
fit.kmeans3 <- kmeans(dat, centers = 3)
fit.kmeans4 <- kmeans(dat, centers = 4)
err3 <- classError(fit.kmeans3$cluster, truth)$errorRate
err4 <- classError(fit.kmeans4$cluster, truth)$errorRate

cat('Clustering error rate of kmeans with k=3: ', err3, '\n')
cat('Clustering error rate of kmeans with k=4: ', err4, '\n')
```

The error rate of keams are $7.91\%$ and $22.0\%$ with $k=3,4$ respectively.

From the reslut above, we know that the density estimation based method did better with appropriate parameters as shown in (4).